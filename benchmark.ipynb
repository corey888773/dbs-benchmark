{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae7c2b-1b56-4aa9-a2bf-d5c1748d744d",
   "metadata": {},
   "source": [
    "# Scenariusze testowe dla porównania wydajności baz danych\n",
    "\n",
    "### 1. Operacja CREATE\n",
    "\n",
    "- Dodanie nowego nauczyciela\n",
    "- Utworzenie nowej klasy\n",
    "- Dodanie nowego przedmiotu\n",
    "- Zarejestrowanie nowego ucznia\n",
    "- Przypisanie ucznia do klasy (**Dodano: Zapisanie ucznia do klasy (enrolment)**)\n",
    "- Utworzenie harmonogramu zajęć\n",
    "- Wystawienie oceny\n",
    "\n",
    "### 2. Operacja READ\n",
    "\n",
    "Pobranie kompleksowego raportu zawierającego:\n",
    "- Dane osobowe ucznia\n",
    "- Informacje o klasie (**Dodano: Informacje o zapisach do klas**)\n",
    "- Dane nauczyciela prowadzącego\n",
    "- Listę ocen z opisami przedmiotów\n",
    "- Szczegółowy harmonogram zajęć\n",
    "\n",
    "### 3. Operacja UPDATE\n",
    "\n",
    "- Aktualizacja danych ucznia\n",
    "- Zmiana przypisania do klasy (**Dodano: Aktualizacja zapisu do klasy**)\n",
    "- Modyfikacja nazwy klasy\n",
    "- Aktualizacja danych nauczyciela\n",
    "- Zmiana oceny\n",
    "- Aktualizacja opisu przedmiotu\n",
    "- Modyfikacja harmonogramu zajęć\n",
    "\n",
    "### 4. Operacja DELETE\n",
    "\n",
    "- Usunięcie ocen ucznia\n",
    "- Wypisanie ucznia z klasy (**Dodano: Usunięcie zapisu do klasy**)\n",
    "- Usunięcie harmonogramu zajęć\n",
    "- Usunięcie klasy\n",
    "- Opcjonalne usunięcie przedmiotów\n",
    "- Opcjonalne usunięcie nauczyciela\n",
    "- Usunięcie rekordu ucznia\n",
    "\n",
    "## Ilość rekordów do testów\n",
    "\n",
    "Testy będą przeprowadzane dla następujących ilości rekordów:\n",
    "\n",
    "1. 10,000 rekordów\n",
    "2. 100,000 rekordów\n",
    "3. 1,000,000 rekordów\n",
    "4. 10,000,000 rekordów\n",
    "\n",
    "## Metryki wydajnościowe\n",
    "\n",
    "Dla każdego scenariusza i ilości rekordów będziemy mierzyć:\n",
    "\n",
    "1. Czas wykonania całego scenariusza\n",
    "2. Średni czas pojedynczych operacji\n",
    "3. Liczbę operacji na sekundę (throughput)\n",
    "4. Zużycie zasobów systemowych (CPU, RAM, I/O dysku)\n",
    "\n",
    "# Narzędzia i technologie testowe\n",
    "\n",
    "### Wbudowane instrumenty bazodanowe\n",
    "\n",
    "Każdy system oferuje specjalizowane narzędzia diagnostyczne:\n",
    "\n",
    "| System | Narzędzie | Funkcjonalności |\n",
    "| :-- | :-- | :-- |\n",
    "| PostgreSQL | pgBench | Testy TPC-B, własne skrypty SQL |\n",
    "| MariaDB | sysbench | Testy OLTP, skalowanie pionowe |\n",
    "| MongoDB | mongoperf | Operacje na dokumentach JSON |\n",
    "| Cassandra | cassandra-stress | Testy dystrybucji danych |\n",
    "| Redis | redis-benchmark | Pomiar opóźnień operacji klucz-wartość |\n",
    "\n",
    "Wykorzystanie natywnych narzędzi pozwala na precyzyjne badanie specyficznych mechanizmów storage engine.\n",
    "\n",
    "### Automatyzacja w Pythonie\n",
    "\n",
    "Kluczowe biblioteki wspierające testy:\n",
    "\n",
    "- **SQLAlchemy** dla baz relacyjnych\n",
    "- **PyMongo** dla MongoDB\n",
    "- **Cassandra-driver** dla Cassandra\n",
    "- **redis-py** dla Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec37216-90ff-435c-9669-67af3f25c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:46:48,666 - WARNING - Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n",
      "2025-04-23 20:46:48,671 - WARNING - Downgrading core protocol version from 66 to 65 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "2025-04-23 20:46:48,674 - WARNING - Downgrading core protocol version from 65 to 5 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "2025-04-23 20:46:48,680 - INFO - Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '::1:9042'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\n",
      "2025-04-23 20:46:48,681 - INFO - Cassandra host 127.0.0.1:9042 removed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database connections...\n",
      "INFO: PostgreSQL connection successful\n",
      "INFO: MariaDB connection successful\n",
      "INFO: Cassandra connection successful\n",
      "INFO: MongoDB connection successful\n",
      "INFO: Redis connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:53:48,757 - WARNING - Host ::1:9042 has been marked down\n",
      "2025-04-23 20:53:49,786 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 1.74 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:53:55,895 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 9.04 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:54:21,915 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 36.48 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import redis\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load database configuration\n",
    "print(\"Setting up database connections...\")\n",
    "with open('docker-compose.yml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "# PostgreSQL connection\n",
    "postgres_config = docker_config['services']['postgresql']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "# MariaDB connection\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "# MongoDB connection\n",
    "mongo_config = docker_config['services']['mongodb']\n",
    "mongo_client = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Cassandra connection\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=cassandra_config['ports'][0].split(':')[0])\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "# Redis connection\n",
    "redis_config = docker_config['services']['redis']\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=int(redis_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Test connections\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 1\")\n",
    "    print(\"INFO: PostgreSQL connection successful\")\n",
    "    \n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    print(\"INFO: MariaDB connection successful\")\n",
    "    \n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    print(\"INFO: Cassandra connection successful\")\n",
    "    \n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"INFO: MongoDB connection successful\")\n",
    "    \n",
    "    redis_client.ping()\n",
    "    print(\"INFO: Redis connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e7675e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd51d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Generating data with scale 100 and batch size 5000...\n",
      "Processing students.csv...\n",
      "  Batch 1/1 processed for students.csv\n",
      "Processing teachers.csv...\n",
      "  Batch 1/1 processed for teachers.csv\n",
      "Processing classes.csv...\n",
      "  Batch 1/1 processed for classes.csv\n",
      "Processing subjects.csv...\n",
      "  Batch 1/1 processed for subjects.csv\n",
      "Processing grades.csv...\n",
      "  Batch 1/1 processed for grades.csv\n",
      "Processing schedules.csv...\n",
      "  Batch 1/1 processed for schedules.csv\n",
      "Processing enrollments.csv...\n",
      "  Batch 1/1 processed for enrollments.csv\n",
      "INFO: Generated 1000 students, 100 teachers, 200 classes, 100 subjects\n",
      "==================================================\n",
      "INFO: Generating data with scale 1000 and batch size 5000...\n",
      "Processing students.csv...\n",
      "  Batch 1/2 processed for students.csv\n",
      "  Batch 2/2 processed for students.csv\n",
      "Processing teachers.csv...\n",
      "  Batch 1/1 processed for teachers.csv\n",
      "Processing classes.csv...\n",
      "  Batch 1/1 processed for classes.csv\n",
      "Processing subjects.csv...\n",
      "  Batch 1/1 processed for subjects.csv\n",
      "Processing grades.csv...\n",
      "  Batch 1/6 processed for grades.csv\n",
      "  Batch 2/6 processed for grades.csv\n",
      "  Batch 3/6 processed for grades.csv\n",
      "  Batch 4/6 processed for grades.csv\n",
      "  Batch 5/6 processed for grades.csv\n",
      "  Batch 6/6 processed for grades.csv\n",
      "Processing schedules.csv...\n",
      "  Batch 1/1 processed for schedules.csv\n",
      "Processing enrollments.csv...\n",
      "  Batch 1/4 processed for enrollments.csv\n",
      "  Batch 2/4 processed for enrollments.csv\n",
      "  Batch 3/4 processed for enrollments.csv\n",
      "  Batch 4/4 processed for enrollments.csv\n",
      "INFO: Generated 10000 students, 1000 teachers, 2000 classes, 1000 subjects\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:53:41,133 - WARNING - Host 127.0.0.1:9042 has been marked down\n",
      "2025-04-23 20:53:41,143 - WARNING - Host 127.0.0.1:9042 has been marked down\n",
      "2025-04-23 20:53:42,193 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 2.26 seconds: [Errno 54] Connection reset by peer\n",
      "2025-04-23 20:53:42,195 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 1.96 seconds: [Errno 54] Connection reset by peer\n",
      "2025-04-23 20:53:48,405 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 7.2 seconds: [Errno 61] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\n",
      "2025-04-23 20:53:48,600 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 8.56 seconds: [Errno 61] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\n",
      "2025-04-23 20:53:48,759 - WARNING - [control connection] Error connecting to ::1:9042:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/cluster.py\", line 3577, in _reconnect_internal\n",
      "    return self._try_connect(host)\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/cluster.py\", line 3599, in _try_connect\n",
      "    connection = self._cluster.connection_factory(host.endpoint, is_control_connection=True)\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/cluster.py\", line 1670, in connection_factory\n",
      "    return self.connection_class.factory(endpoint, self.connect_timeout, *args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/connection.py\", line 846, in factory\n",
      "    conn = cls(endpoint, *args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/io/libevreactor.py\", line 267, in __init__\n",
      "    self._connect_socket()\n",
      "  File \"/opt/miniconda3/envs/db-benchmark/lib/python3.10/site-packages/cassandra/connection.py\", line 951, in _connect_socket\n",
      "    raise socket.error(sockerr.errno, \"Tried connecting to %s. Last error: %s\" %\n",
      "ConnectionRefusedError: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:53:51,542 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 4.32 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:54:05,029 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 16.8 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:54:11,438 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 33.6 seconds: [Errno 61] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\n",
      "2025-04-23 20:54:12,451 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 30.08 seconds: [Errno 61] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\n",
      "2025-04-23 20:54:58,494 - WARNING - Error attempting to reconnect to ::1:9042, scheduling retry in 62.08 seconds: [Errno 61] Tried connecting to [('::1', 9042, 0, 0)]. Last error: Connection refused\n",
      "2025-04-23 20:55:40,822 - WARNING - Error attempting to reconnect to 127.0.0.1:9042, scheduling retry in 111.36 seconds: [Errno 61] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\n"
     ]
    }
   ],
   "source": [
    "# Data generation functions\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from generator import generate_school_data\n",
    "\n",
    "def generate_files(output_dir='./data', scale=1000, batch_size=10000, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate synthetic school data files for benchmarking.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Generating data with scale {scale} and batch size {batch_size}...\")\n",
    "\n",
    "    result = generate_school_data(\n",
    "        output_dir=output_dir,\n",
    "        scale=scale,\n",
    "        batch_size=batch_size,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"INFO: Generated {len(result['students'])} students, {len(result['teachers'])} teachers, \" + \n",
    "          f\"{len(result['classes'])} classes, {len(result['subjects'])} subjects\")\n",
    "    print(\"=\"*50)\n",
    "    return result\n",
    "\n",
    "# Generate test data sets\n",
    "scale_100_dir = './data/scale_100'\n",
    "scale_1000_dir = './data/scale_1000'\n",
    "\n",
    "generate_files(output_dir=scale_100_dir, scale=100, batch_size=5000)\n",
    "generate_files(output_dir=scale_1000_dir, scale=1000, batch_size=5000)\n",
    "STOP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dd2d0",
   "metadata": {},
   "source": [
    "# PostgreSQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "1d62cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Methods\n",
    "\n",
    "def initialize_postgres_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the PostgreSQL database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "        print(\"INFO: PostgreSQL schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing PostgreSQL schema: {e}\")\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in PostgreSQL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_postgres_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    table_csv_map = {\n",
    "        'teachers': 'teachers.csv',\n",
    "        'subjects': 'subjects.csv',\n",
    "        'classes': 'classes.csv',\n",
    "        'students': 'students.csv',\n",
    "        'grades': 'grades.csv',\n",
    "        'schedules': 'schedules.csv',\n",
    "        'enrollments': 'enrollments.csv'\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"INFO: Loading PostgreSQL data from {data_dir}\")\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name, csv_file in table_csv_map.items():\n",
    "                file_path = data_path / csv_file\n",
    "                if not file_path.exists():\n",
    "                    print(f\"WARNING: CSV file not found: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                load_start = time.time()\n",
    "\n",
    "                if table_name == 'enrollments':\n",
    "                    # Special handling for enrollments using temp table\n",
    "                    print(f\"INFO: Loading enrollments with duplicate handling...\")\n",
    "                    temp_table_name = f\"temp_{table_name}\"\n",
    "                    try:\n",
    "                        cur.execute(f\"\"\"\n",
    "                            CREATE TEMP TABLE {temp_table_name} (\n",
    "                                student_id INT,\n",
    "                                class_id INT,\n",
    "                                enrolled_at TIMESTAMP\n",
    "                            ) ON COMMIT DROP;\n",
    "                        \"\"\")\n",
    "                        copy_sql = f\"COPY {temp_table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            cur.copy_expert(sql=copy_sql, file=f)\n",
    "\n",
    "                        insert_sql = f\"\"\"\n",
    "                            INSERT INTO {table_name} (student_id, class_id, enrolled_at)\n",
    "                            SELECT student_id, class_id, enrolled_at FROM {temp_table_name}\n",
    "                            ON CONFLICT (student_id, class_id) DO NOTHING;\n",
    "                        \"\"\"\n",
    "                        cur.execute(insert_sql)\n",
    "                        inserted_count = cur.rowcount\n",
    "                        conn.commit()\n",
    "                        load_end = time.time()\n",
    "                        print(f\"INFO: Loaded {inserted_count} enrollments in {load_end - load_start:.2f} seconds.\")\n",
    "                    except Exception as enroll_error:\n",
    "                        conn.rollback()\n",
    "                        print(f\"ERROR: {enroll_error}\")\n",
    "                else:\n",
    "                    # Standard COPY for other tables\n",
    "                    print(f\"INFO: Loading {table_name}...\")\n",
    "                    copy_sql = f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "                    try:\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            cur.copy_expert(sql=copy_sql, file=f)\n",
    "                        conn.commit()\n",
    "                        load_end = time.time()\n",
    "                        print(f\"INFO: Loaded {table_name} in {load_end - load_start:.2f} seconds.\")\n",
    "                    except Exception as copy_error:\n",
    "                        conn.rollback()\n",
    "                        print(f\"ERROR: {copy_error}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print(f\"INFO: PostgreSQL loading complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "def verify_postgres_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in PostgreSQL tables\")\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- PostgreSQL Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "897d53c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PostgreSQL schema initialized.\n",
      "INFO: All PostgreSQL tables exist: teachers, subjects, classes, students, enrollments, grades, schedules\n",
      "INFO: Loading PostgreSQL data from ./data/scale_100\n",
      "INFO: Loading teachers...\n",
      "INFO: Loaded teachers in 0.00 seconds.\n",
      "INFO: Loading subjects...\n",
      "INFO: Loaded subjects in 0.00 seconds.\n",
      "INFO: Loading classes...\n",
      "INFO: Loaded classes in 0.00 seconds.\n",
      "INFO: Loading students...\n",
      "INFO: Loaded students in 0.00 seconds.\n",
      "INFO: Loading grades...\n",
      "INFO: Loaded grades in 0.02 seconds.\n",
      "INFO: Loading schedules...\n",
      "INFO: Loaded schedules in 0.01 seconds.\n",
      "INFO: Loading enrollments with duplicate handling...\n",
      "INFO: Loaded 1993 enrollments in 0.02 seconds.\n",
      "INFO: PostgreSQL loading complete in 0.06 seconds.\n",
      "INFO: Counting rows in PostgreSQL tables\n",
      "--- PostgreSQL Table Row Counts ---\n",
      "teachers    : 100\n",
      "subjects    : 100\n",
      "classes     : 200\n",
      "students    : 1000\n",
      "enrollments : 1993\n",
      "grades      : 3000\n",
      "schedules   : 500\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PostgreSQL Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/postgres_schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "\n",
    "# Table verification \n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_postgres_tables(postgres_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_postgres_data(postgres_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_postgres_counts(postgres_client, required_tables)\n",
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721535d9",
   "metadata": {},
   "source": [
    "# MariaDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "5249a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB Methods\n",
    "\n",
    "def initialize_mariadb_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the MariaDB database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for statement in schema_sql.split(';'):\n",
    "                stmt = statement.strip()\n",
    "                if stmt:\n",
    "                    cur.execute(stmt)\n",
    "        conn.commit()\n",
    "        print(\"INFO: MariaDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing MariaDB schema: {e}\")\n",
    "\n",
    "def verify_mariadb_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in MariaDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            format_strings = ','.join(['%s'] * len(expected_tables))\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = DATABASE() AND table_name IN ({format_strings});\n",
    "            \"\"\", tuple(expected_tables))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All MariaDB tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MariaDB tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying MariaDB tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_mariadb_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into MariaDB tables.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    table_csv_map = {\n",
    "        'teachers': 'teachers.csv',\n",
    "        'subjects': 'subjects.csv',\n",
    "        'classes': 'classes.csv',\n",
    "        'students': 'students.csv',\n",
    "        'grades': 'grades.csv',\n",
    "        'schedules': 'schedules.csv',\n",
    "        'enrollments': 'enrollments.csv'\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"INFO: Loading MariaDB data from {data_dir}\")\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name, csv_file in table_csv_map.items():\n",
    "                file_path = data_path / csv_file\n",
    "                if not file_path.exists():\n",
    "                    print(f\"WARNING: CSV file not found: {file_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                load_start = time.time()\n",
    "                \n",
    "                try:\n",
    "                    if table_name == 'enrollments':\n",
    "                        # Handle enrollments with INSERT IGNORE to skip duplicates\n",
    "                        print(f\"INFO: Loading enrollments with duplicate handling...\")\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            next(f)  # skip header\n",
    "                            for line in f:\n",
    "                                student_id, class_id, enrolled_at = line.strip().split(',')\n",
    "                                cur.execute(\n",
    "                                    \"\"\"\n",
    "                                    INSERT IGNORE INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                                    VALUES (%s, %s, %s)\n",
    "                                    \"\"\",\n",
    "                                    (student_id, class_id, enrolled_at)\n",
    "                                )\n",
    "                        conn.commit()\n",
    "                    else:\n",
    "                        # Use LOAD DATA LOCAL INFILE for other tables\n",
    "                        print(f\"INFO: Loading {table_name}...\")\n",
    "                        load_sql = f\"\"\"\n",
    "                        LOAD DATA LOCAL INFILE '{file_path.resolve()}'\n",
    "                        INTO TABLE {table_name}\n",
    "                        FIELDS TERMINATED BY ','\n",
    "                        OPTIONALLY ENCLOSED BY '\"'\n",
    "                        LINES TERMINATED BY '\\n'\n",
    "                        IGNORE 1 LINES;\n",
    "                        \"\"\"\n",
    "                        cur.execute(load_sql)\n",
    "                        conn.commit()\n",
    "                        \n",
    "                    load_end = time.time()\n",
    "                    print(f\"INFO: Loaded {table_name} in {load_end - load_start:.2f} seconds.\")\n",
    "                except Exception as load_error:\n",
    "                    conn.rollback()\n",
    "                    print(f\"ERROR: {load_error}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print(f\"INFO: MariaDB loading complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "def verify_mariadb_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in MariaDB tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in MariaDB tables\")\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- MariaDB Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"---------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "85f5ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MariaDB schema initialized.\n",
      "INFO: All MariaDB tables exist: teachers, subjects, classes, students, enrollments, grades, schedules\n",
      "INFO: Loading MariaDB data from ./data/scale_100\n",
      "INFO: Loading teachers...\n",
      "INFO: Loaded teachers in 0.00 seconds.\n",
      "INFO: Loading subjects...\n",
      "INFO: Loaded subjects in 0.00 seconds.\n",
      "INFO: Loading classes...\n",
      "INFO: Loaded classes in 0.00 seconds.\n",
      "INFO: Loading students...\n",
      "INFO: Loaded students in 0.00 seconds.\n",
      "INFO: Loading grades...\n",
      "INFO: Loaded grades in 0.01 seconds.\n",
      "INFO: Loading schedules...\n",
      "INFO: Loaded schedules in 0.00 seconds.\n",
      "INFO: Loading enrollments with duplicate handling...\n",
      "INFO: Loaded enrollments in 0.28 seconds.\n",
      "INFO: MariaDB loading complete in 0.31 seconds.\n",
      "INFO: Counting rows in MariaDB tables\n",
      "--- MariaDB Table Row Counts ---\n",
      "teachers    : 100\n",
      "subjects    : 100\n",
      "classes     : 200\n",
      "students    : 1000\n",
      "enrollments : 1993\n",
      "grades      : 3000\n",
      "schedules   : 500\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MariaDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/mariadb_schema.sql', 'r') as f:\n",
    "    mariadb_schema = f.read()\n",
    "\n",
    "initialize_mariadb_schema(mariadb_client, mariadb_schema)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_mariadb_tables(mariadb_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_mariadb_data(mariadb_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_mariadb_counts(mariadb_client, required_tables)\n",
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eb7b3",
   "metadata": {},
   "source": [
    "# MongoDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "f2f0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Methods\n",
    "\n",
    "def initialize_mongo_schema(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Initializes the MongoDB schema by creating necessary collections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        \n",
    "        # List of collections to create based on no_sql_design.txt\n",
    "        collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "        \n",
    "        # Drop existing collections if they exist\n",
    "        for collection in collections:\n",
    "            if collection in db.list_collection_names():\n",
    "                db[collection].drop()\n",
    "                print(f\"INFO: Dropped MongoDB collection: {collection}\")\n",
    "        \n",
    "        # Create collections with indexes\n",
    "        for collection in collections:\n",
    "            db.create_collection(collection)\n",
    "            print(f\"INFO: Created MongoDB collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for performance\n",
    "            if collection == 'students':\n",
    "                db[collection].create_index([(\"last_name\", 1), (\"first_name\", 1)])\n",
    "            elif collection == 'classes':\n",
    "                db[collection].create_index([(\"name\", 1)])\n",
    "                \n",
    "        print(\"INFO: MongoDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_mongo_collections(client, db_name='benchmark', expected_collections=None):\n",
    "    \"\"\"\n",
    "    Verifies if the expected collections exist in MongoDB.\n",
    "    \"\"\"\n",
    "    if expected_collections is None:\n",
    "        expected_collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        missing_collections = set(expected_collections) - set(existing_collections)\n",
    "        if not missing_collections:\n",
    "            print(f\"INFO: All MongoDB collections exist: {', '.join(expected_collections)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MongoDB collections: {', '.join(missing_collections)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_mongo_data(client, data_dir, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into MongoDB collections with document-oriented structure.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"INFO: Loading MongoDB data from {data_dir}\")\n",
    "    \n",
    "    # Clear previous data\n",
    "    for collection in ['students', 'teachers', 'classes', 'subjects']:\n",
    "        db[collection].delete_many({})\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load teachers\n",
    "        teachers_file = data_path / 'teachers.csv'\n",
    "        teachers_map = {}\n",
    "        \n",
    "        if teachers_file.exists():\n",
    "            print(f\"INFO: Loading teachers...\")\n",
    "            load_start = time.time()\n",
    "            \n",
    "            teachers = []\n",
    "            with open(teachers_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    teacher_doc = {\n",
    "                        \"_id\": row['id'],\n",
    "                        \"first_name\": row['first_name'],\n",
    "                        \"last_name\": row['last_name'],\n",
    "                        \"subject\": row['subject'],\n",
    "                        \"hire_date\": row['hire_date']\n",
    "                    }\n",
    "                    teachers.append(teacher_doc)\n",
    "                    teachers_map[row['id']] = f\"{row['first_name']} {row['last_name']}\"\n",
    "                    \n",
    "            if teachers:\n",
    "                db.teachers.insert_many(teachers)\n",
    "                load_end = time.time()\n",
    "                print(f\"INFO: Loaded {len(teachers)} teachers in {load_end - load_start:.2f} seconds.\")\n",
    "        \n",
    "        # Step 2: Load subjects\n",
    "        subjects_file = data_path / 'subjects.csv'\n",
    "        subjects_map = {}\n",
    "        \n",
    "        if subjects_file.exists():\n",
    "            print(f\"INFO: Loading subjects...\")\n",
    "            load_start = time.time()\n",
    "            \n",
    "            subjects = []\n",
    "            with open(subjects_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    subject_doc = {\n",
    "                        \"_id\": row['id'],\n",
    "                        \"name\": row['name'],\n",
    "                        \"description\": row['description']\n",
    "                    }\n",
    "                    subjects.append(subject_doc)\n",
    "                    subjects_map[row['id']] = row['name']\n",
    "                    \n",
    "            if subjects:\n",
    "                db.subjects.insert_many(subjects)\n",
    "                load_end = time.time()\n",
    "                print(f\"INFO: Loaded {len(subjects)} subjects in {load_end - load_start:.2f} seconds.\")\n",
    "        \n",
    "        # Step 3: Process schedules for embedding in classes\n",
    "        schedules_file = data_path / 'schedules.csv'\n",
    "        schedules_map = {}\n",
    "        \n",
    "        if schedules_file.exists():\n",
    "            print(f\"INFO: Processing schedules...\")\n",
    "            with open(schedules_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    class_id = row['class_id']\n",
    "                    if class_id not in schedules_map:\n",
    "                        schedules_map[class_id] = []\n",
    "                        \n",
    "                    schedules_map[class_id].append({\n",
    "                        \"subject_id\": row['subject_id'],\n",
    "                        \"day_of_week\": row['day_of_week'],\n",
    "                        \"time_start\": row['time_start'],\n",
    "                        \"time_end\": row['time_end']\n",
    "                    })\n",
    "        \n",
    "        # Step 4: Load classes with embedded schedules\n",
    "        classes_file = data_path / 'classes.csv'\n",
    "        classes_map = {}\n",
    "        \n",
    "        if classes_file.exists():\n",
    "            print(f\"INFO: Loading classes with embedded schedules...\")\n",
    "            load_start = time.time()\n",
    "            \n",
    "            classes = []\n",
    "            with open(classes_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    class_id = row['id']\n",
    "                    teacher_id = row['teacher_id']\n",
    "                    \n",
    "                    class_doc = {\n",
    "                        \"_id\": class_id,\n",
    "                        \"name\": row['name'],\n",
    "                        \"teacher\": {\n",
    "                            \"teacher_id\": teacher_id,\n",
    "                            \"name\": teachers_map.get(teacher_id, \"Unknown\")\n",
    "                        },\n",
    "                        \"schedule\": schedules_map.get(class_id, [])\n",
    "                    }\n",
    "                    \n",
    "                    classes.append(class_doc)\n",
    "                    classes_map[class_id] = row['name']\n",
    "                    \n",
    "            if classes:\n",
    "                db.classes.insert_many(classes)\n",
    "                load_end = time.time()\n",
    "                print(f\"INFO: Loaded {len(classes)} classes in {load_end - load_start:.2f} seconds.\")\n",
    "        \n",
    "        # Step 5: Process enrollments and grades for embedding in students\n",
    "        enrollments_file = data_path / 'enrollments.csv'\n",
    "        enrollments_map = {}\n",
    "        \n",
    "        if enrollments_file.exists():\n",
    "            print(f\"INFO: Processing enrollments...\")\n",
    "            with open(enrollments_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    student_id = row['student_id']\n",
    "                    if student_id not in enrollments_map:\n",
    "                        enrollments_map[student_id] = []\n",
    "                        \n",
    "                    enrollments_map[student_id].append({\n",
    "                        \"class_id\": row['class_id'],\n",
    "                        \"enrolled_at\": row['enrolled_at']\n",
    "                    })\n",
    "        \n",
    "        grades_file = data_path / 'grades.csv'\n",
    "        grades_map = {}\n",
    "        \n",
    "        if grades_file.exists():\n",
    "            print(f\"INFO: Processing grades...\")\n",
    "            with open(grades_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    student_id = row['student_id']\n",
    "                    if student_id not in grades_map:\n",
    "                        grades_map[student_id] = []\n",
    "                        \n",
    "                    grades_map[student_id].append({\n",
    "                        \"subject_id\": row['subject_id'],\n",
    "                        \"grade\": row['grade'],\n",
    "                        \"created_at\": row['created_at']\n",
    "                    })\n",
    "        \n",
    "        # Step 6: Load students with embedded enrollments and grades\n",
    "        students_file = data_path / 'students.csv'\n",
    "        \n",
    "        if students_file.exists():\n",
    "            print(f\"INFO: Loading students with embedded enrollments and grades...\")\n",
    "            load_start = time.time()\n",
    "            \n",
    "            # Process in batches\n",
    "            batch_size = 5000\n",
    "            batch = []\n",
    "            student_count = 0\n",
    "            \n",
    "            with open(students_file, 'r') as f:\n",
    "                reader = pd.read_csv(f)\n",
    "                for _, row in reader.iterrows():\n",
    "                    student_id = row['id']\n",
    "                    student_doc = {\n",
    "                        \"_id\": student_id,\n",
    "                        \"first_name\": row['first_name'],\n",
    "                        \"last_name\": row['last_name'],\n",
    "                        \"birth_date\": row['birth_date'],\n",
    "                        \"enrollments\": enrollments_map.get(student_id, []),\n",
    "                        \"grades\": grades_map.get(student_id, [])\n",
    "                    }\n",
    "                    \n",
    "                    batch.append(student_doc)\n",
    "                    student_count += 1\n",
    "                    \n",
    "                    # Insert batch when it reaches batch_size\n",
    "                    if len(batch) >= batch_size:\n",
    "                        db.students.insert_many(batch)\n",
    "                        batch = []\n",
    "                \n",
    "                # Insert any remaining documents\n",
    "                if batch:\n",
    "                    db.students.insert_many(batch)\n",
    "                    \n",
    "                load_end = time.time()\n",
    "                print(f\"INFO: Loaded {student_count} students in {load_end - load_start:.2f} seconds.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print(f\"INFO: MongoDB loading complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "def verify_mongo_counts(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Counts documents in MongoDB collections.\n",
    "    \"\"\"\n",
    "    collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    max_len = max(len(c) for c in collections)\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        counts = {}\n",
    "        \n",
    "        for collection in collections:\n",
    "            try:\n",
    "                count = db[collection].count_documents({})\n",
    "                counts[collection] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[collection] = 'Error'\n",
    "                \n",
    "        print(\"--- MongoDB Collection Document Counts ---\")\n",
    "        for collection, count in counts.items():\n",
    "            print(f\"{collection:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "        # Additional checks for embedded documents\n",
    "        try:\n",
    "            students_with_enrollments = db.students.count_documents({\"enrollments\": {\"$exists\": True, \"$ne\": []}})\n",
    "            students_with_grades = db.students.count_documents({\"grades\": {\"$exists\": True, \"$ne\": []}})\n",
    "            classes_with_schedules = db.classes.count_documents({\"schedule\": {\"$exists\": True, \"$ne\": []}})\n",
    "            \n",
    "            print(\"\\n--- MongoDB Embedded Document Counts ---\")\n",
    "            print(f\"Students with enrollments : {students_with_enrollments}\")\n",
    "            print(f\"Students with grades      : {students_with_grades}\")\n",
    "            print(f\"Classes with schedules    : {classes_with_schedules}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "f14fdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dropped MongoDB collection: students\n",
      "INFO: Dropped MongoDB collection: teachers\n",
      "INFO: Dropped MongoDB collection: classes\n",
      "INFO: Dropped MongoDB collection: subjects\n",
      "INFO: Created MongoDB collection: students\n",
      "INFO: Created MongoDB collection: teachers\n",
      "INFO: Created MongoDB collection: classes\n",
      "INFO: Created MongoDB collection: subjects\n",
      "INFO: MongoDB schema initialized.\n",
      "INFO: All MongoDB collections exist: students, teachers, classes, subjects\n",
      "INFO: Loading MongoDB data from ./data/scale_100\n",
      "INFO: Loading teachers...\n",
      "INFO: Loaded 100 teachers in 0.00 seconds.\n",
      "INFO: Loading subjects...\n",
      "INFO: Loaded 100 subjects in 0.00 seconds.\n",
      "INFO: Processing schedules...\n",
      "INFO: Loading classes with embedded schedules...\n",
      "INFO: Loaded 200 classes in 0.01 seconds.\n",
      "INFO: Processing enrollments...\n",
      "INFO: Processing grades...\n",
      "INFO: Loading students with embedded enrollments and grades...\n",
      "INFO: Loaded 1000 students in 0.03 seconds.\n",
      "INFO: MongoDB loading complete in 0.13 seconds.\n",
      "--- MongoDB Collection Document Counts ---\n",
      "students : 1000\n",
      "teachers : 100\n",
      "classes  : 200\n",
      "subjects : 100\n",
      "-----------------------------------------\n",
      "\n",
      "--- MongoDB Embedded Document Counts ---\n",
      "Students with enrollments : 853\n",
      "Students with grades      : 937\n",
      "Classes with schedules    : 177\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MongoDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_mongo_schema(mongo_client)\n",
    "\n",
    "# Collection verification\n",
    "verify_mongo_collections(mongo_client)\n",
    "\n",
    "# Data loading\n",
    "load_mongo_data(mongo_client, scale_100_dir)\n",
    "\n",
    "# Document count verification\n",
    "verify_mongo_counts(mongo_client)\n",
    "STOP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
