{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae7c2b-1b56-4aa9-a2bf-d5c1748d744d",
   "metadata": {},
   "source": [
    "# Scenariusze testowe dla porównania wydajności baz danych\n",
    "\n",
    "### 1. Operacja CREATE\n",
    "\n",
    "- Dodanie nowego nauczyciela\n",
    "- Utworzenie nowej klasy\n",
    "- Dodanie nowego przedmiotu\n",
    "- Zarejestrowanie nowego ucznia\n",
    "- Przypisanie ucznia do klasy (**Dodano: Zapisanie ucznia do klasy (enrolment)**)\n",
    "- Utworzenie harmonogramu zajęć\n",
    "- Wystawienie oceny\n",
    "\n",
    "### 2. Operacja READ\n",
    "\n",
    "Pobranie kompleksowego raportu zawierającego:\n",
    "- Dane osobowe ucznia\n",
    "- Informacje o klasie (**Dodano: Informacje o zapisach do klas**)\n",
    "- Dane nauczyciela prowadzącego\n",
    "- Listę ocen z opisami przedmiotów\n",
    "- Szczegółowy harmonogram zajęć\n",
    "\n",
    "### 3. Operacja UPDATE\n",
    "\n",
    "- Aktualizacja danych ucznia\n",
    "- Zmiana przypisania do klasy (**Dodano: Aktualizacja zapisu do klasy**)\n",
    "- Modyfikacja nazwy klasy\n",
    "- Aktualizacja danych nauczyciela\n",
    "- Zmiana oceny\n",
    "- Aktualizacja opisu przedmiotu\n",
    "- Modyfikacja harmonogramu zajęć\n",
    "\n",
    "### 4. Operacja DELETE\n",
    "\n",
    "- Usunięcie ocen ucznia\n",
    "- Wypisanie ucznia z klasy (**Dodano: Usunięcie zapisu do klasy**)\n",
    "- Usunięcie harmonogramu zajęć\n",
    "- Usunięcie klasy\n",
    "- Opcjonalne usunięcie przedmiotów\n",
    "- Opcjonalne usunięcie nauczyciela\n",
    "- Usunięcie rekordu ucznia\n",
    "\n",
    "## Ilość rekordów do testów\n",
    "\n",
    "Testy będą przeprowadzane dla następujących ilości rekordów:\n",
    "\n",
    "1. 10,000 rekordów\n",
    "2. 100,000 rekordów\n",
    "3. 1,000,000 rekordów\n",
    "4. 10,000,000 rekordów\n",
    "\n",
    "## Metryki wydajnościowe\n",
    "\n",
    "Dla każdego scenariusza i ilości rekordów będziemy mierzyć:\n",
    "\n",
    "1. Czas wykonania całego scenariusza\n",
    "2. Średni czas pojedynczych operacji\n",
    "3. Liczbę operacji na sekundę (throughput)\n",
    "4. Zużycie zasobów systemowych (CPU, RAM, I/O dysku)\n",
    "\n",
    "# Narzędzia i technologie testowe\n",
    "\n",
    "### Wbudowane instrumenty bazodanowe\n",
    "\n",
    "Każdy system oferuje specjalizowane narzędzia diagnostyczne:\n",
    "\n",
    "| System | Narzędzie | Funkcjonalności |\n",
    "| :-- | :-- | :-- |\n",
    "| PostgreSQL | pgBench | Testy TPC-B, własne skrypty SQL |\n",
    "| MariaDB | sysbench | Testy OLTP, skalowanie pionowe |\n",
    "| MongoDB | mongoperf | Operacje na dokumentach JSON |\n",
    "| Cassandra | cassandra-stress | Testy dystrybucji danych |\n",
    "| Redis | redis-benchmark | Pomiar opóźnień operacji klucz-wartość |\n",
    "\n",
    "Wykorzystanie natywnych narzędzi pozwala na precyzyjne badanie specyficznych mechanizmów storage engine.\n",
    "\n",
    "### Automatyzacja w Pythonie\n",
    "\n",
    "Kluczowe biblioteki wspierające testy:\n",
    "\n",
    "- **SQLAlchemy** dla baz relacyjnych\n",
    "- **PyMongo** dla MongoDB\n",
    "- **Cassandra-driver** dla Cassandra\n",
    "- **redis-py** dla Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec37216-90ff-435c-9669-67af3f25c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import redis\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load database configuration\n",
    "print(\"Setting up database connections...\")\n",
    "with open('docker-compose.yml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "# PostgreSQL connection\n",
    "postgres_config = docker_config['services']['postgresql']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "# MariaDB connection\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "# MongoDB connection\n",
    "mongo_config = docker_config['services']['mongodb']\n",
    "mongo_client = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Cassandra connection\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=cassandra_config['ports'][0].split(':')[0])\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "# Redis connection\n",
    "redis_config = docker_config['services']['redis']\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=int(redis_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Test connections\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 1\")\n",
    "    print(\"INFO: PostgreSQL connection successful\")\n",
    "    \n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    print(\"INFO: MariaDB connection successful\")\n",
    "    \n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    print(\"INFO: Cassandra connection successful\")\n",
    "    \n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"INFO: MongoDB connection successful\")\n",
    "    \n",
    "    redis_client.ping()\n",
    "    print(\"INFO: Redis connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7675e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_END = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd51d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation functions\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from generator import generate_school_data\n",
    "\n",
    "def generate_files(output_dir='./data', scale=1000, batch_size=10000, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate synthetic school data files for benchmarking.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Generating data with scale {scale} and batch size {batch_size}...\")\n",
    "\n",
    "    result = generate_school_data(\n",
    "        output_dir=output_dir,\n",
    "        scale=scale,\n",
    "        batch_size=batch_size,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"INFO: Generated {len(result['students'])} students, {len(result['teachers'])} teachers, \" + \n",
    "          f\"{len(result['classes'])} classes, {len(result['subjects'])} subjects\")\n",
    "    print(\"=\"*50)\n",
    "    return result\n",
    "\n",
    "# Generate test data sets\n",
    "scale_100_dir = './data/scale_100'\n",
    "scale_1000_dir = './data/scale_1000'\n",
    "\n",
    "generate_files(output_dir=scale_100_dir, scale=100, batch_size=5000)\n",
    "generate_files(output_dir=scale_1000_dir, scale=1000, batch_size=5000)\n",
    "CELL_END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dd2d0",
   "metadata": {},
   "source": [
    "# PostgreSQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Methods\n",
    "\n",
    "def initialize_postgres_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the PostgreSQL database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "        print(\"INFO: PostgreSQL schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing PostgreSQL schema: {e}\")\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in PostgreSQL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using INSERT.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    insert_sql = f\"INSERT INTO {table_name} VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except psycopg2.errors.UniqueViolation:\n",
    "                        # Ignore duplicate key errors\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create a temporary table for the COPY operation\n",
    "            temp_table_name = \"temp_enrollments\"\n",
    "            cur.execute(f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table_name} (\n",
    "                    student_id INT,\n",
    "                    class_id INT,\n",
    "                    enrolled_at TIMESTAMP\n",
    "                ) ON COMMIT DROP;\n",
    "            \"\"\")\n",
    "            copy_sql = f\"COPY {temp_table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time()\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "\n",
    "            # Insert into the main table, ignoring duplicates\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                SELECT student_id, class_id, enrolled_at FROM {temp_table_name}\n",
    "                ON CONFLICT (student_id, class_id) DO NOTHING;\n",
    "            \"\"\"\n",
    "            cur.execute(insert_sql)\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_postgres_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    table_csv_map = {\n",
    "        'teachers': 'teachers.csv',\n",
    "        'subjects': 'subjects.csv',\n",
    "        'classes': 'classes.csv',\n",
    "        'students': 'students.csv',\n",
    "        'grades': 'grades.csv',\n",
    "        'schedules': 'schedules.csv',\n",
    "        # 'enrollments': 'enrollments.csv' # Handled separately\n",
    "    }\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = copy_postgres_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_postgres_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_postgres_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in PostgreSQL tables\")\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- PostgreSQL Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/postgres_schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "\n",
    "# Table verification \n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_postgres_tables(postgres_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_postgres_data(postgres_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_postgres_counts(postgres_client, required_tables)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721535d9",
   "metadata": {},
   "source": [
    "# MariaDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB Methods\n",
    "\n",
    "def initialize_mariadb_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the MariaDB database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for statement in schema_sql.split(';'):\n",
    "                stmt = statement.strip()\n",
    "                if stmt:\n",
    "                    cur.execute(stmt)\n",
    "        conn.commit()\n",
    "        print(\"INFO: MariaDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing MariaDB schema: {e}\")\n",
    "\n",
    "def verify_mariadb_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in MariaDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            format_strings = ','.join(['%s'] * len(expected_tables))\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = DATABASE() AND table_name IN ({format_strings});\n",
    "            \"\"\", tuple(expected_tables))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All MariaDB tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MariaDB tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying MariaDB tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    \"\"\"Inserts data from a CSV file into a MariaDB table by reading the header for columns.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                # read header for column names\n",
    "                header = next(f).strip().split(',')\n",
    "                cols = header\n",
    "                placeholders = ','.join(['%s'] * len(cols))\n",
    "                insert_sql = f\"INSERT INTO {table_name} ({','.join(cols)}) VALUES ({placeholders})\"\n",
    "                file_opened_start_time = time.time()\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    # ensure values length matches columns\n",
    "                    if len(values) != len(cols):\n",
    "                        if len(values) > len(cols):\n",
    "                            values = values[:len(cols)]\n",
    "                        else:\n",
    "                            print(f\"WARNING: Skipping {table_name} row with {len(values)} values (expected {len(cols)}, values: {values})\")\n",
    "                            continue\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except mysql.connector.errors.IntegrityError:\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "\n",
    "def copy_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a MariaDB table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"\"\"\n",
    "            LOAD DATA LOCAL INFILE '{csv_file}'\n",
    "            INTO TABLE {table_name}\n",
    "            FIELDS TERMINATED BY ','\n",
    "            OPTIONALLY ENCLOSED BY '\"'\n",
    "            LINES TERMINATED BY '\\n'\n",
    "            IGNORE 1 LINES;\n",
    "            \"\"\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.execute(copy_sql)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_mariadb_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Handle enrollments with INSERT IGNORE to skip duplicates\n",
    "            print(f\"INFO: Loading enrollments with duplicate handling...\")\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # skip header\n",
    "                for line in f:\n",
    "                    student_id, class_id, enrolled_at = line.strip().split(',')\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT IGNORE INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        \"\"\",\n",
    "                        (student_id, class_id, enrolled_at)\n",
    "                    )\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_mariadb_data(conn, data_dir):\n",
    "    table_csv_map = {\n",
    "    'teachers': 'teachers.csv',\n",
    "    'subjects': 'subjects.csv',\n",
    "    'classes': 'classes.csv',\n",
    "    'students': 'students.csv',\n",
    "    'grades': 'grades.csv',\n",
    "    'schedules': 'schedules.csv',\n",
    "    # 'enrollments': 'enrollments.csv' Handled separately\n",
    "    }\n",
    "    data_path = Path(data_dir)\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = insert_mariadb_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_mariadb_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_mariadb_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in MariaDB tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in MariaDB tables\")\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- MariaDB Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"---------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/mariadb_schema.sql', 'r') as f:\n",
    "    mariadb_schema = f.read()\n",
    "\n",
    "initialize_mariadb_schema(mariadb_client, mariadb_schema)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_mariadb_tables(mariadb_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_mariadb_data(mariadb_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_mariadb_counts(mariadb_client, required_tables)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eb7b3",
   "metadata": {},
   "source": [
    "# MongoDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Methods\n",
    "def initialize_mongo_schema(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Initializes the MongoDB schema by creating necessary collections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        \n",
    "        # List of collections to create based on no_sql_design.txt\n",
    "        collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "        \n",
    "        # Drop existing collections if they exist\n",
    "        for collection in collections:\n",
    "            if collection in db.list_collection_names():\n",
    "                db[collection].drop()\n",
    "                print(f\"INFO: Dropped MongoDB collection: {collection}\")\n",
    "        \n",
    "        # Create collections with indexes\n",
    "        for collection in collections:\n",
    "            db.create_collection(collection)\n",
    "            print(f\"INFO: Created MongoDB collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for performance\n",
    "            if collection == 'students':\n",
    "                db[collection].create_index([(\"last_name\", 1), (\"first_name\", 1)])\n",
    "            elif collection == 'classes':\n",
    "                db[collection].create_index([(\"name\", 1)])\n",
    "                \n",
    "        print(\"INFO: MongoDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_mongo_collections(client, db_name='benchmark', expected_collections=None):\n",
    "    \"\"\"\n",
    "    Verifies if the expected collections exist in MongoDB.\n",
    "    \"\"\"\n",
    "    if expected_collections is None:\n",
    "        expected_collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        missing_collections = set(expected_collections) - set(existing_collections)\n",
    "        if not missing_collections:\n",
    "            print(f\"INFO: All MongoDB collections exist: {', '.join(expected_collections)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MongoDB collections: {', '.join(missing_collections)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mongo_data_from_csv(client, collection_name, csv_file) -> tuple[float, float, float]:\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            # rename id to _id for MongoDB\n",
    "            if 'id' in reader.columns:\n",
    "                reader.rename(columns={'id': '_id'}, inplace=True)\n",
    "\n",
    "            file_opened_start_time = time.time() # Initialize start_time just before starting to insert\n",
    "            for _, row in reader.iterrows():\n",
    "                doc = row.to_dict()\n",
    "                collection.insert_one(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_mongo_students_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all grades and enrollments into students from csv files\n",
    "    # create a student object with embedded enrollments and grades\n",
    "    students_file = data_path / 'students.csv'\n",
    "    enrollments_file = data_path / 'enrollments.csv'\n",
    "    grades_file = data_path / 'grades.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['students']\n",
    "        \n",
    "        with open(students_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"first_name\": row['first_name'],\n",
    "                    \"last_name\": row['last_name'],\n",
    "                    \"birth_date\": row['birth_date'],\n",
    "                    \"enrollments\": [],\n",
    "                    \"grades\": []\n",
    "                }\n",
    "                collection.insert_one(student_doc)\n",
    "\n",
    "        with open(enrollments_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                enrollment_doc = {\n",
    "                    \"class_id\": row['class_id'],\n",
    "                    \"enrolled_at\": row['enrolled_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"enrollments\": enrollment_doc}}\n",
    "                )\n",
    "\n",
    "        with open(grades_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                grade_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"grade\": row['grade'],\n",
    "                    \"created_at\": row['created_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"grades\": grade_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "\n",
    "def insert_mongo_classes_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all teachers and schedules into classes from csv files\n",
    "    # create a class object with embedded teachers and schedules\n",
    "    classes_file = data_path / 'classes.csv'\n",
    "    schedules_file = data_path / 'schedules.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['classes']\n",
    "\n",
    "        with open(classes_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"teacher_id\": row['teacher_id'],\n",
    "                    \"schedule\": []\n",
    "                }\n",
    "                collection.insert_one(class_doc)\n",
    "\n",
    "        with open(schedules_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_id = row['class_id']\n",
    "                schedule_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"day_of_week\": row['day_of_week'],\n",
    "                    \"time_start\": row['time_start'],\n",
    "                    \"time_end\": row['time_end']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": class_id},\n",
    "                    {\"$push\": {\"schedule\": schedule_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "def load_mongo_data(client, data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    insert_mongo_data_from_csv(client, 'teachers', data_path / 'teachers.csv')\n",
    "    insert_mongo_data_from_csv(client, 'subjects', data_path / 'subjects.csv')\n",
    "    insert_mongo_students_from_csv(client, data_path)\n",
    "    insert_mongo_classes_from_csv(client, data_path)\n",
    "\n",
    "def verify_mongo_counts(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Counts documents in MongoDB collections.\n",
    "    \"\"\"\n",
    "    collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    max_len = max(len(c) for c in collections)\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        counts = {}\n",
    "        \n",
    "        for collection in collections:\n",
    "            try:\n",
    "                count = db[collection].count_documents({})\n",
    "                counts[collection] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[collection] = 'Error'\n",
    "                \n",
    "        print(\"--- MongoDB Collection Document Counts ---\")\n",
    "        for collection, count in counts.items():\n",
    "            print(f\"{collection:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "        # Additional checks for embedded documents\n",
    "        try:\n",
    "            students_with_enrollments = db.students.count_documents({\"enrollments\": {\"$exists\": True, \"$ne\": []}})\n",
    "            students_with_grades = db.students.count_documents({\"grades\": {\"$exists\": True, \"$ne\": []}})\n",
    "            classes_with_schedules = db.classes.count_documents({\"schedule\": {\"$exists\": True, \"$ne\": []}})\n",
    "            \n",
    "            print(\"\\n--- MongoDB Embedded Document Counts ---\")\n",
    "            print(f\"Students with enrollments : {students_with_enrollments}\")\n",
    "            print(f\"Students with grades      : {students_with_grades}\")\n",
    "            print(f\"Classes with schedules    : {classes_with_schedules}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fdc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_mongo_schema(mongo_client)\n",
    "\n",
    "# Collection verification\n",
    "verify_mongo_collections(mongo_client)\n",
    "\n",
    "# Data loading\n",
    "load_mongo_data(mongo_client, scale_100_dir)\n",
    "\n",
    "# Document count verification\n",
    "verify_mongo_counts(mongo_client)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eff04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra data loading functions with minimal memory usage\n",
    "\n",
    "def initialize_cassandra_schema(session, keyspace='benchmark'):\n",
    "    \"\"\"Initializes the Cassandra schema by creating necessary keyspace and tables.\"\"\"\n",
    "    try:\n",
    "        # Create keyspace if not exists\n",
    "        session.execute(f\"\"\"\n",
    "            CREATE KEYSPACE IF NOT EXISTS {keyspace} \n",
    "            WITH REPLICATION = {{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }};\n",
    "        \"\"\")\n",
    "        \n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Drop existing tables if they exist\n",
    "        tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                    'enrollments', 'grades', 'schedules', \n",
    "                    'student_enrollments', 'student_grades']\n",
    "        \n",
    "        for table in tables:\n",
    "            session.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "            print(f\"INFO: Dropped Cassandra table: {table}\")\n",
    "        \n",
    "        # Create tables with appropriate data types\n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE teachers (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE subjects (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                description TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE classes (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                teacher_id INT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE students (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT,\n",
    "                birth_date TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE grades (\n",
    "                id INT PRIMARY KEY,\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE schedules (\n",
    "                id INT PRIMARY KEY,\n",
    "                class_id INT,\n",
    "                subject_id INT,\n",
    "                day_of_week INT,\n",
    "                time_start TEXT,\n",
    "                time_end TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                class_name TEXT,\n",
    "                teacher_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_grades (\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                subject_name TEXT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, subject_id, created_at)\n",
    "            ) WITH CLUSTERING ORDER BY (subject_id ASC, created_at DESC);\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"INFO: Cassandra schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_cassandra_tables(session, keyspace='benchmark', expected_tables=None):\n",
    "    \"\"\"Verifies if the expected tables exist in Cassandra.\"\"\"\n",
    "    if expected_tables is None:\n",
    "        expected_tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                            'enrollments', 'grades', 'schedules']\n",
    "    \n",
    "    try:\n",
    "        # Get existing tables\n",
    "        query = f\"\"\"\n",
    "            SELECT table_name FROM system_schema.tables \n",
    "            WHERE keyspace_name = '{keyspace}';\n",
    "        \"\"\"\n",
    "        rows = session.execute(query)\n",
    "        existing_tables = {row.table_name for row in rows}\n",
    "        \n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All Cassandra tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing Cassandra tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_cassandra_teachers(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert teacher data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO teachers (id, first_name, last_name) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2]             # last_name\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted teachers successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load teachers: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_subjects(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert subject data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO subjects (id, name, description) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        values[2]             # description\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted subjects successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load subjects: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_classes(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert class data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO classes (id, name, teacher_id) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        int(values[2])        # teacher_id\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted classes successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load classes: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_students(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert student data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO students (id, first_name, last_name, birth_date) VALUES (?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2],            # last_name\n",
    "                        values[3]             # birth_date\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted students successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load students: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_enrollments(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert enrollment data from CSV, line by line, with timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    enrolled_at = datetime.fromisoformat(values[2].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # student_id\n",
    "                        int(values[1]),       # class_id\n",
    "                        enrolled_at           # enrolled_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted enrollments successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load enrollments: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_grades(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert grades data from CSV, line by line, with UUID and timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    created_at = datetime.fromisoformat(values[4].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # student_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        float(values[3]),     # grade\n",
    "                        created_at            # created_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted grades successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load grades: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_schedules(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert schedule data from CSV, line by line, with UUID and day mapping.\"\"\"\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Day name to integer mapping\n",
    "        day_map = {\n",
    "            'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, \n",
    "            'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7\n",
    "        }\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 6:  # Ensure we have enough columns\n",
    "                    # Convert day name to integer\n",
    "                    day_num = day_map.get(values[3], 0)\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # class_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        day_num,              # day_of_week as int\n",
    "                        values[4],            # time_start\n",
    "                        values[5]             # time_end\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted schedules successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load schedules: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def populate_cassandra_denormalized_tables(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"\n",
    "    Populate denormalized tables for efficient queries.\n",
    "    This requires more memory as we need to join data in Python.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # --- Populate student_enrollments table ---\n",
    "        # This requires joining enrollments with classes\n",
    "        enrollments_df = pd.read_csv(data_path / 'enrollments.csv')\n",
    "        classes_df = pd.read_csv(data_path / 'classes.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_enrollments (student_id, class_id, class_name, teacher_id, enrolled_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process\n",
    "        merged = pd.merge(enrollments_df, classes_df, left_on='class_id', right_on='id')\n",
    "        for _, row in merged.iterrows():\n",
    "            enrolled_at = datetime.fromisoformat(row['enrolled_at'].replace('Z', '+00:00'))\n",
    "            \n",
    "            session.execute(stmt, [\n",
    "                int(row['student_id']),\n",
    "                int(row['class_id']),\n",
    "                row['name'],\n",
    "                int(row['teacher_id']),\n",
    "                enrolled_at\n",
    "            ])\n",
    "        \n",
    "        print(\"INFO: Populated student_enrollments denormalized table\")\n",
    "        \n",
    "        # --- Populate student_grades table ---\n",
    "        # This requires joining grades with subjects\n",
    "        grades_df = pd.read_csv(data_path / 'grades.csv')\n",
    "        subjects_df = pd.read_csv(data_path / 'subjects.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_grades (student_id, subject_id, subject_name, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process\n",
    "        merged = pd.merge(grades_df, subjects_df, left_on='subject_id', right_on='id')\n",
    "        for _, row in merged.iterrows():\n",
    "            created_at = datetime.fromisoformat(row['created_at_x'].replace('Z', '+00:00'))\n",
    "            \n",
    "            session.execute(stmt, [\n",
    "                int(row['student_id']),\n",
    "                int(row['subject_id']),\n",
    "                row['name'],\n",
    "                float(row['grade']),\n",
    "                created_at\n",
    "            ])\n",
    "        \n",
    "        print(\"INFO: Populated student_grades denormalized table\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to populate denormalized tables: {e}\")\n",
    "\n",
    "def load_cassandra_data(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"Load all data into Cassandra tables.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Use the keyspace\n",
    "    session.execute(f\"USE {keyspace};\")\n",
    "    \n",
    "    # Insert basic entities\n",
    "    op_time, f_op_time, end_time = insert_cassandra_teachers(session, data_path / 'teachers.csv')\n",
    "    print(f\"INFO: Inserted teachers in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_subjects(session, data_path / 'subjects.csv')\n",
    "    print(f\"INFO: Inserted subjects in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_classes(session, data_path / 'classes.csv')\n",
    "    print(f\"INFO: Inserted classes in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_students(session, data_path / 'students.csv')\n",
    "    print(f\"INFO: Inserted students in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Insert relationships and complex data\n",
    "    op_time, f_op_time, end_time = insert_cassandra_enrollments(session, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_grades(session, data_path / 'grades.csv')\n",
    "    print(f\"INFO: Inserted grades in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_schedules(session, data_path / 'schedules.csv')\n",
    "    print(f\"INFO: Inserted schedules in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Populate denormalized tables\n",
    "    populate_cassandra_denormalized_tables(session, data_path)\n",
    "\n",
    "def verify_cassandra_counts(session, keyspace='benchmark'):\n",
    "    \"\"\"Count rows in all Cassandra tables.\"\"\"\n",
    "    tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                'enrollments', 'grades', 'schedules', \n",
    "                'student_enrollments', 'student_grades']\n",
    "    max_len = max(len(t) for t in tables)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        counts = {}\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                rows = session.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                count = rows.one()[0]\n",
    "                counts[table] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[table] = 'Error'\n",
    "        \n",
    "        print(\"--- Cassandra Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"----------------------------------\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_cassandra_schema(cassandra_session)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_cassandra_tables(cassandra_session, expected_tables=required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_cassandra_data(cassandra_session, scale_100_dir)\n",
    "\n",
    "# Row count verification\n",
    "verify_cassandra_counts(cassandra_session)\n",
    "CELL_END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
