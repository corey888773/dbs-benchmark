{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae7c2b-1b56-4aa9-a2bf-d5c1748d744d",
   "metadata": {},
   "source": [
    "# Scenariusze testowe dla porównania wydajności baz danych\n",
    "\n",
    "### 1. Operacja CREATE\n",
    "\n",
    "- Dodanie nowego nauczyciela\n",
    "- Utworzenie nowej klasy\n",
    "- Dodanie nowego przedmiotu\n",
    "- Zarejestrowanie nowego ucznia\n",
    "- Przypisanie ucznia do klasy (**Dodano: Zapisanie ucznia do klasy (enrolment)**)\n",
    "- Utworzenie harmonogramu zajęć\n",
    "- Wystawienie oceny\n",
    "\n",
    "### 2. Operacja READ\n",
    "\n",
    "Pobranie kompleksowego raportu zawierającego:\n",
    "- Dane osobowe ucznia\n",
    "- Informacje o klasie (**Dodano: Informacje o zapisach do klas**)\n",
    "- Dane nauczyciela prowadzącego\n",
    "- Listę ocen z opisami przedmiotów\n",
    "- Szczegółowy harmonogram zajęć\n",
    "\n",
    "### 3. Operacja UPDATE\n",
    "\n",
    "- Aktualizacja danych ucznia\n",
    "- Zmiana przypisania do klasy (**Dodano: Aktualizacja zapisu do klasy**)\n",
    "- Modyfikacja nazwy klasy\n",
    "- Aktualizacja danych nauczyciela\n",
    "- Zmiana oceny\n",
    "- Aktualizacja opisu przedmiotu\n",
    "- Modyfikacja harmonogramu zajęć\n",
    "\n",
    "### 4. Operacja DELETE\n",
    "\n",
    "- Usunięcie ocen ucznia\n",
    "- Wypisanie ucznia z klasy (**Dodano: Usunięcie zapisu do klasy**)\n",
    "- Usunięcie harmonogramu zajęć\n",
    "- Usunięcie klasy\n",
    "- Opcjonalne usunięcie przedmiotów\n",
    "- Opcjonalne usunięcie nauczyciela\n",
    "- Usunięcie rekordu ucznia\n",
    "\n",
    "## Ilość rekordów do testów\n",
    "\n",
    "Testy będą przeprowadzane dla następujących ilości rekordów:\n",
    "\n",
    "1. 10,000 rekordów\n",
    "2. 100,000 rekordów\n",
    "3. 1,000,000 rekordów\n",
    "4. 10,000,000 rekordów\n",
    "\n",
    "## Metryki wydajnościowe\n",
    "\n",
    "Dla każdego scenariusza i ilości rekordów będziemy mierzyć:\n",
    "\n",
    "1. Czas wykonania całego scenariusza\n",
    "2. Średni czas pojedynczych operacji\n",
    "3. Liczbę operacji na sekundę (throughput)\n",
    "4. Zużycie zasobów systemowych (CPU, RAM, I/O dysku)\n",
    "\n",
    "# Narzędzia i technologie testowe\n",
    "\n",
    "### Wbudowane instrumenty bazodanowe\n",
    "\n",
    "Każdy system oferuje specjalizowane narzędzia diagnostyczne:\n",
    "\n",
    "| System | Narzędzie | Funkcjonalności |\n",
    "| :-- | :-- | :-- |\n",
    "| PostgreSQL | pgBench | Testy TPC-B, własne skrypty SQL |\n",
    "| MariaDB | sysbench | Testy OLTP, skalowanie pionowe |\n",
    "| MongoDB | mongoperf | Operacje na dokumentach JSON |\n",
    "| Cassandra | cassandra-stress | Testy dystrybucji danych |\n",
    "| Redis | redis-benchmark | Pomiar opóźnień operacji klucz-wartość |\n",
    "\n",
    "Wykorzystanie natywnych narzędzi pozwala na precyzyjne badanie specyficznych mechanizmów storage engine.\n",
    "\n",
    "### Automatyzacja w Pythonie\n",
    "\n",
    "Kluczowe biblioteki wspierające testy:\n",
    "\n",
    "- **SQLAlchemy** dla baz relacyjnych\n",
    "- **PyMongo** dla MongoDB\n",
    "- **Cassandra-driver** dla Cassandra\n",
    "- **redis-py** dla Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec37216-90ff-435c-9669-67af3f25c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database connections...\n",
      "INFO: PostgreSQL connection successful\n",
      "INFO: MariaDB connection successful\n",
      "INFO: Cassandra connection successful\n",
      "INFO: MongoDB connection successful\n",
      "INFO: Redis connection successful\n",
      "INFO: PostgreSQL connection successful\n",
      "INFO: MariaDB connection successful\n",
      "INFO: Cassandra connection successful\n",
      "INFO: MongoDB connection successful\n",
      "INFO: Redis connection successful\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import redis\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load database configuration\n",
    "print(\"Setting up database connections...\")\n",
    "with open('docker-compose.yml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "# PostgreSQL connection\n",
    "postgres_config = docker_config['services']['postgresql']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "# MariaDB connection\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "# MongoDB connection\n",
    "mongo_config = docker_config['services']['mongodb']\n",
    "mongo_client = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Cassandra connection\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=cassandra_config['ports'][0].split(':')[0])\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "# Redis connection\n",
    "redis_config = docker_config['services']['redis']\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=int(redis_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Test connections\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 1\")\n",
    "    print(\"INFO: PostgreSQL connection successful\")\n",
    "    \n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    print(\"INFO: MariaDB connection successful\")\n",
    "    \n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    print(\"INFO: Cassandra connection successful\")\n",
    "    \n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"INFO: MongoDB connection successful\")\n",
    "    \n",
    "    redis_client.ping()\n",
    "    print(\"INFO: Redis connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7675e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_END = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd51d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Generating data with scale 100 and batch size 5000...\n",
      "Processing students.csv...\n",
      "  Batch 1/1 processed for students.csv\n",
      "Processing teachers.csv...\n",
      "  Batch 1/1 processed for teachers.csv\n",
      "Processing classes.csv...\n",
      "  Batch 1/1 processed for classes.csv\n",
      "Processing subjects.csv...\n",
      "  Batch 1/1 processed for subjects.csv\n",
      "Processing grades.csv...\n",
      "  Batch 1/1 processed for grades.csv\n",
      "Processing schedules.csv...\n",
      "  Batch 1/1 processed for schedules.csv\n",
      "Processing enrollments.csv...\n",
      "  Batch 1/1 processed for enrollments.csv\n",
      "INFO: Generated 1000 students, 100 teachers, 200 classes, 100 subjects\n",
      "==================================================\n",
      "INFO: Generating data with scale 1000 and batch size 5000...\n",
      "Processing students.csv...\n",
      "  Batch 1/2 processed for students.csv\n",
      "  Batch 2/2 processed for students.csv\n",
      "Processing teachers.csv...\n",
      "  Batch 1/1 processed for teachers.csv\n",
      "Processing classes.csv...\n",
      "  Batch 1/1 processed for classes.csv\n",
      "Processing subjects.csv...\n",
      "  Batch 1/1 processed for subjects.csv\n",
      "Processing grades.csv...\n",
      "  Batch 1/6 processed for grades.csv\n",
      "  Batch 2/6 processed for grades.csv\n",
      "  Batch 3/6 processed for grades.csv\n",
      "  Batch 4/6 processed for grades.csv\n",
      "  Batch 5/6 processed for grades.csv\n",
      "  Batch 6/6 processed for grades.csv\n",
      "Processing schedules.csv...\n",
      "  Batch 1/1 processed for schedules.csv\n",
      "Processing enrollments.csv...\n",
      "  Batch 1/4 processed for enrollments.csv\n",
      "  Batch 2/4 processed for enrollments.csv\n",
      "  Batch 3/4 processed for enrollments.csv\n",
      "  Batch 4/4 processed for enrollments.csv\n",
      "INFO: Generated 10000 students, 1000 teachers, 2000 classes, 1000 subjects\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data generation functions\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from generator import generate_school_data\n",
    "\n",
    "def generate_files(output_dir='./data', scale=1000, batch_size=10000, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate synthetic school data files for benchmarking.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Generating data with scale {scale} and batch size {batch_size}...\")\n",
    "\n",
    "    result = generate_school_data(\n",
    "        output_dir=output_dir,\n",
    "        scale=scale,\n",
    "        batch_size=batch_size,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"INFO: Generated {len(result['students'])} students, {len(result['teachers'])} teachers, \" + \n",
    "          f\"{len(result['classes'])} classes, {len(result['subjects'])} subjects\")\n",
    "    print(\"=\"*50)\n",
    "    return result\n",
    "\n",
    "# Generate test data sets\n",
    "scale_100_dir = './data/scale_100'\n",
    "scale_1000_dir = './data/scale_1000'\n",
    "\n",
    "generate_files(output_dir=scale_100_dir, scale=100, batch_size=5000)\n",
    "generate_files(output_dir=scale_1000_dir, scale=1000, batch_size=5000)\n",
    "CELL_END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dd2d0",
   "metadata": {},
   "source": [
    "# PostgreSQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d62cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Methods\n",
    "\n",
    "def initialize_postgres_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the PostgreSQL database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "        print(\"INFO: PostgreSQL schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing PostgreSQL schema: {e}\")\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in PostgreSQL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using INSERT.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    insert_sql = f\"INSERT INTO {table_name} VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except psycopg2.errors.UniqueViolation:\n",
    "                        # Ignore duplicate key errors\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create a temporary table for the COPY operation\n",
    "            temp_table_name = \"temp_enrollments\"\n",
    "            cur.execute(f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table_name} (\n",
    "                    student_id INT,\n",
    "                    class_id INT,\n",
    "                    enrolled_at TIMESTAMP\n",
    "                ) ON COMMIT DROP;\n",
    "            \"\"\")\n",
    "            copy_sql = f\"COPY {temp_table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time()\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "\n",
    "            # Insert into the main table, ignoring duplicates\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                SELECT student_id, class_id, enrolled_at FROM {temp_table_name}\n",
    "                ON CONFLICT (student_id, class_id) DO NOTHING;\n",
    "            \"\"\"\n",
    "            cur.execute(insert_sql)\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_postgres_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    table_csv_map = {\n",
    "        'teachers': 'teachers.csv',\n",
    "        'subjects': 'subjects.csv',\n",
    "        'classes': 'classes.csv',\n",
    "        'students': 'students.csv',\n",
    "        'grades': 'grades.csv',\n",
    "        'schedules': 'schedules.csv',\n",
    "        # 'enrollments': 'enrollments.csv' # Handled separately\n",
    "    }\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = copy_postgres_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_postgres_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_postgres_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in PostgreSQL tables\")\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- PostgreSQL Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897d53c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PostgreSQL schema initialized.\n",
      "INFO: All PostgreSQL tables exist: teachers, subjects, classes, students, enrollments, grades, schedules\n",
      "INFO: Inserted teachers in 0.00 seconds (file opened in 0.00 seconds)\n",
      "INFO: Inserted subjects in 0.00 seconds (file opened in 0.00 seconds)\n",
      "INFO: Inserted classes in 0.00 seconds (file opened in 0.00 seconds)\n",
      "INFO: Inserted students in 0.00 seconds (file opened in 0.00 seconds)\n",
      "INFO: Inserted grades in 0.02 seconds (file opened in 0.02 seconds)\n",
      "INFO: Inserted schedules in 0.00 seconds (file opened in 0.00 seconds)\n",
      "INFO: Inserted enrollments in 0.02 seconds (file opened in 0.02 seconds)\n",
      "INFO: Counting rows in PostgreSQL tables\n",
      "--- PostgreSQL Table Row Counts ---\n",
      "teachers    : 100\n",
      "subjects    : 100\n",
      "classes     : 200\n",
      "students    : 1000\n",
      "enrollments : 1994\n",
      "grades      : 3000\n",
      "schedules   : 500\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PostgreSQL Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/postgres_schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "\n",
    "# Table verification \n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_postgres_tables(postgres_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_postgres_data(postgres_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_postgres_counts(postgres_client, required_tables)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721535d9",
   "metadata": {},
   "source": [
    "# MariaDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5249a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB Methods\n",
    "\n",
    "def initialize_mariadb_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the MariaDB database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for statement in schema_sql.split(';'):\n",
    "                stmt = statement.strip()\n",
    "                if stmt:\n",
    "                    cur.execute(stmt)\n",
    "        conn.commit()\n",
    "        print(\"INFO: MariaDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing MariaDB schema: {e}\")\n",
    "\n",
    "def verify_mariadb_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in MariaDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            format_strings = ','.join(['%s'] * len(expected_tables))\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = DATABASE() AND table_name IN ({format_strings});\n",
    "            \"\"\", tuple(expected_tables))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All MariaDB tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MariaDB tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying MariaDB tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    \"\"\"Inserts data from a CSV file into a MariaDB table by reading the header for columns.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                # read header for column names\n",
    "                header = next(f).strip().split(',')\n",
    "                cols = header\n",
    "                placeholders = ','.join(['%s'] * len(cols))\n",
    "                insert_sql = f\"INSERT INTO {table_name} ({','.join(cols)}) VALUES ({placeholders})\"\n",
    "                file_opened_start_time = time.time()\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    # ensure values length matches columns\n",
    "                    if len(values) != len(cols):\n",
    "                        if len(values) > len(cols):\n",
    "                            values = values[:len(cols)]\n",
    "                        else:\n",
    "                            print(f\"WARNING: Skipping {table_name} row with {len(values)} values (expected {len(cols)}, values: {values})\")\n",
    "                            continue\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except mysql.connector.errors.IntegrityError:\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "\n",
    "def copy_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a MariaDB table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"\"\"\n",
    "            LOAD DATA LOCAL INFILE '{csv_file}'\n",
    "            INTO TABLE {table_name}\n",
    "            FIELDS TERMINATED BY ','\n",
    "            OPTIONALLY ENCLOSED BY '\"'\n",
    "            LINES TERMINATED BY '\\n'\n",
    "            IGNORE 1 LINES;\n",
    "            \"\"\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.execute(copy_sql)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_mariadb_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Handle enrollments with INSERT IGNORE to skip duplicates\n",
    "            print(f\"INFO: Loading enrollments with duplicate handling...\")\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # skip header\n",
    "                for line in f:\n",
    "                    student_id, class_id, enrolled_at = line.strip().split(',')\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT IGNORE INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        \"\"\",\n",
    "                        (student_id, class_id, enrolled_at)\n",
    "                    )\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_mariadb_data(conn, data_dir):\n",
    "    table_csv_map = {\n",
    "    'teachers': 'teachers.csv',\n",
    "    'subjects': 'subjects.csv',\n",
    "    'classes': 'classes.csv',\n",
    "    'students': 'students.csv',\n",
    "    'grades': 'grades.csv',\n",
    "    'schedules': 'schedules.csv',\n",
    "    # 'enrollments': 'enrollments.csv' Handled separately\n",
    "    }\n",
    "    data_path = Path(data_dir)\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = insert_mariadb_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_mariadb_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_mariadb_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in MariaDB tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in MariaDB tables\")\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- MariaDB Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"---------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f5ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: MariaDB schema initialized.\n",
      "INFO: All MariaDB tables exist: teachers, subjects, classes, students, enrollments, grades, schedules\n",
      "INFO: Inserted teachers in 0.01 seconds (file opened in 0.01 seconds)\n",
      "INFO: Inserted subjects in 0.01 seconds (file opened in 0.01 seconds)\n",
      "INFO: Inserted classes in 0.03 seconds (file opened in 0.03 seconds)\n",
      "INFO: Inserted students in 0.13 seconds (file opened in 0.13 seconds)\n",
      "INFO: Inserted grades in 0.38 seconds (file opened in 0.38 seconds)\n",
      "INFO: Inserted schedules in 0.06 seconds (file opened in 0.06 seconds)\n",
      "INFO: Loading enrollments with duplicate handling...\n",
      "INFO: Inserted enrollments in 0.27 seconds (file opened in 1746445937.27 seconds)\n",
      "INFO: Counting rows in MariaDB tables\n",
      "--- MariaDB Table Row Counts ---\n",
      "teachers    : 100\n",
      "subjects    : 100\n",
      "classes     : 200\n",
      "students    : 1000\n",
      "enrollments : 1994\n",
      "grades      : 3000\n",
      "schedules   : 500\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MariaDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/mariadb_schema.sql', 'r') as f:\n",
    "    mariadb_schema = f.read()\n",
    "\n",
    "initialize_mariadb_schema(mariadb_client, mariadb_schema)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_mariadb_tables(mariadb_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_mariadb_data(mariadb_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_mariadb_counts(mariadb_client, required_tables)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eb7b3",
   "metadata": {},
   "source": [
    "# MongoDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Methods\n",
    "def initialize_mongo_schema(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Initializes the MongoDB schema by creating necessary collections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        \n",
    "        # List of collections to create based on no_sql_design.txt\n",
    "        collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "        \n",
    "        # Drop existing collections if they exist\n",
    "        for collection in collections:\n",
    "            if collection in db.list_collection_names():\n",
    "                db[collection].drop()\n",
    "                print(f\"INFO: Dropped MongoDB collection: {collection}\")\n",
    "        \n",
    "        # Create collections with indexes\n",
    "        for collection in collections:\n",
    "            db.create_collection(collection)\n",
    "            print(f\"INFO: Created MongoDB collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for performance\n",
    "            if collection == 'students':\n",
    "                db[collection].create_index([(\"last_name\", 1), (\"first_name\", 1)])\n",
    "            elif collection == 'classes':\n",
    "                db[collection].create_index([(\"name\", 1)])\n",
    "                \n",
    "        print(\"INFO: MongoDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_mongo_collections(client, db_name='benchmark', expected_collections=None):\n",
    "    \"\"\"\n",
    "    Verifies if the expected collections exist in MongoDB.\n",
    "    \"\"\"\n",
    "    if expected_collections is None:\n",
    "        expected_collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        missing_collections = set(expected_collections) - set(existing_collections)\n",
    "        if not missing_collections:\n",
    "            print(f\"INFO: All MongoDB collections exist: {', '.join(expected_collections)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MongoDB collections: {', '.join(missing_collections)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mongo_data_from_csv(client, collection_name, csv_file) -> tuple[float, float, float]:\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            # rename id to _id for MongoDB\n",
    "            if 'id' in reader.columns:\n",
    "                reader.rename(columns={'id': '_id'}, inplace=True)\n",
    "\n",
    "            file_opened_start_time = time.time() # Initialize start_time just before starting to insert\n",
    "            for _, row in reader.iterrows():\n",
    "                doc = row.to_dict()\n",
    "                collection.insert_one(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_mongo_students_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all grades and enrollments into students from csv files\n",
    "    # create a student object with embedded enrollments and grades\n",
    "    students_file = data_path / 'students.csv'\n",
    "    enrollments_file = data_path / 'enrollments.csv'\n",
    "    grades_file = data_path / 'grades.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['students']\n",
    "        \n",
    "        with open(students_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"first_name\": row['first_name'],\n",
    "                    \"last_name\": row['last_name'],\n",
    "                    \"birth_date\": row['birth_date'],\n",
    "                    \"enrollments\": [],\n",
    "                    \"grades\": []\n",
    "                }\n",
    "                collection.insert_one(student_doc)\n",
    "\n",
    "        with open(enrollments_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                enrollment_doc = {\n",
    "                    \"class_id\": row['class_id'],\n",
    "                    \"enrolled_at\": row['enrolled_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"enrollments\": enrollment_doc}}\n",
    "                )\n",
    "\n",
    "        with open(grades_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                grade_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"grade\": row['grade'],\n",
    "                    \"created_at\": row['created_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"grades\": grade_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "\n",
    "def insert_mongo_classes_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all teachers and schedules into classes from csv files\n",
    "    # create a class object with embedded teachers and schedules\n",
    "    classes_file = data_path / 'classes.csv'\n",
    "    schedules_file = data_path / 'schedules.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['classes']\n",
    "\n",
    "        with open(classes_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"teacher_id\": row['teacher_id'],\n",
    "                    \"schedule\": []\n",
    "                }\n",
    "                collection.insert_one(class_doc)\n",
    "\n",
    "        with open(schedules_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_id = row['class_id']\n",
    "                schedule_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"day_of_week\": row['day_of_week'],\n",
    "                    \"time_start\": row['time_start'],\n",
    "                    \"time_end\": row['time_end']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": class_id},\n",
    "                    {\"$push\": {\"schedule\": schedule_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "def load_mongo_data(client, data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    insert_mongo_data_from_csv(client, 'teachers', data_path / 'teachers.csv')\n",
    "    insert_mongo_data_from_csv(client, 'subjects', data_path / 'subjects.csv')\n",
    "    insert_mongo_students_from_csv(client, data_path)\n",
    "    insert_mongo_classes_from_csv(client, data_path)\n",
    "\n",
    "def verify_mongo_counts(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Counts documents in MongoDB collections.\n",
    "    \"\"\"\n",
    "    collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    max_len = max(len(c) for c in collections)\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        counts = {}\n",
    "        \n",
    "        for collection in collections:\n",
    "            try:\n",
    "                count = db[collection].count_documents({})\n",
    "                counts[collection] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[collection] = 'Error'\n",
    "                \n",
    "        print(\"--- MongoDB Collection Document Counts ---\")\n",
    "        for collection, count in counts.items():\n",
    "            print(f\"{collection:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "        # Additional checks for embedded documents\n",
    "        try:\n",
    "            students_with_enrollments = db.students.count_documents({\"enrollments\": {\"$exists\": True, \"$ne\": []}})\n",
    "            students_with_grades = db.students.count_documents({\"grades\": {\"$exists\": True, \"$ne\": []}})\n",
    "            classes_with_schedules = db.classes.count_documents({\"schedule\": {\"$exists\": True, \"$ne\": []}})\n",
    "            \n",
    "            print(\"\\n--- MongoDB Embedded Document Counts ---\")\n",
    "            print(f\"Students with enrollments : {students_with_enrollments}\")\n",
    "            print(f\"Students with grades      : {students_with_grades}\")\n",
    "            print(f\"Classes with schedules    : {classes_with_schedules}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14fdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dropped MongoDB collection: students\n",
      "INFO: Dropped MongoDB collection: teachers\n",
      "INFO: Dropped MongoDB collection: classes\n",
      "INFO: Dropped MongoDB collection: subjects\n",
      "INFO: Created MongoDB collection: students\n",
      "INFO: Created MongoDB collection: teachers\n",
      "INFO: Created MongoDB collection: classes\n",
      "INFO: Created MongoDB collection: subjects\n",
      "INFO: MongoDB schema initialized.\n",
      "INFO: All MongoDB collections exist: students, teachers, classes, subjects\n",
      "--- MongoDB Collection Document Counts ---\n",
      "students : 1000\n",
      "teachers : 100\n",
      "classes  : 200\n",
      "subjects : 100\n",
      "-----------------------------------------\n",
      "\n",
      "--- MongoDB Embedded Document Counts ---\n",
      "Students with enrollments : 867\n",
      "Students with grades      : 944\n",
      "Classes with schedules    : 189\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MongoDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_mongo_schema(mongo_client)\n",
    "\n",
    "# Collection verification\n",
    "verify_mongo_collections(mongo_client)\n",
    "\n",
    "# Data loading\n",
    "load_mongo_data(mongo_client, scale_100_dir)\n",
    "\n",
    "# Document count verification\n",
    "verify_mongo_counts(mongo_client)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32eff04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra data loading functions with minimal memory usage\n",
    "\n",
    "def initialize_cassandra_schema(session, keyspace='benchmark'):\n",
    "    \"\"\"Initializes the Cassandra schema by creating necessary keyspace and tables.\"\"\"\n",
    "    try:\n",
    "        # Create keyspace if not exists\n",
    "        session.execute(f\"\"\"\n",
    "            CREATE KEYSPACE IF NOT EXISTS {keyspace} \n",
    "            WITH REPLICATION = {{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }};\n",
    "        \"\"\")\n",
    "        \n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Drop existing tables if they exist\n",
    "        tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                    'enrollments', 'grades', 'schedules', \n",
    "                    'student_enrollments', 'student_grades']\n",
    "        \n",
    "        for table in tables:\n",
    "            session.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "            print(f\"INFO: Dropped Cassandra table: {table}\")\n",
    "        \n",
    "        # Create tables with appropriate data types\n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE teachers (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE subjects (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                description TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE classes (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                teacher_id INT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE students (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT,\n",
    "                birth_date TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE grades (\n",
    "                id INT PRIMARY KEY,\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE schedules (\n",
    "                id INT PRIMARY KEY,\n",
    "                class_id INT,\n",
    "                subject_id INT,\n",
    "                day_of_week INT,\n",
    "                time_start TEXT,\n",
    "                time_end TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                class_name TEXT,\n",
    "                teacher_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_grades (\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                subject_name TEXT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, subject_id, created_at)\n",
    "            ) WITH CLUSTERING ORDER BY (subject_id ASC, created_at DESC);\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"INFO: Cassandra schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_cassandra_tables(session, keyspace='benchmark', expected_tables=None):\n",
    "    \"\"\"Verifies if the expected tables exist in Cassandra.\"\"\"\n",
    "    if expected_tables is None:\n",
    "        expected_tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                            'enrollments', 'grades', 'schedules']\n",
    "    \n",
    "    try:\n",
    "        # Get existing tables\n",
    "        query = f\"\"\"\n",
    "            SELECT table_name FROM system_schema.tables \n",
    "            WHERE keyspace_name = '{keyspace}';\n",
    "        \"\"\"\n",
    "        rows = session.execute(query)\n",
    "        existing_tables = {row.table_name for row in rows}\n",
    "        \n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All Cassandra tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing Cassandra tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_cassandra_teachers(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert teacher data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO teachers (id, first_name, last_name) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2]             # last_name\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted teachers successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load teachers: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_subjects(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert subject data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO subjects (id, name, description) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        values[2]             # description\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted subjects successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load subjects: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_classes(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert class data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO classes (id, name, teacher_id) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        int(values[2])        # teacher_id\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted classes successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load classes: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_students(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert student data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO students (id, first_name, last_name, birth_date) VALUES (?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2],            # last_name\n",
    "                        values[3]             # birth_date\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted students successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load students: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_enrollments(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert enrollment data from CSV, line by line, with timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    enrolled_at = datetime.fromisoformat(values[2].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # student_id\n",
    "                        int(values[1]),       # class_id\n",
    "                        enrolled_at           # enrolled_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted enrollments successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load enrollments: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_grades(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert grades data from CSV, line by line, with UUID and timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    created_at = datetime.fromisoformat(values[4].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # student_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        float(values[3]),     # grade\n",
    "                        created_at            # created_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted grades successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load grades: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_schedules(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert schedule data from CSV, line by line, with UUID and day mapping.\"\"\"\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Day name to integer mapping\n",
    "        day_map = {\n",
    "            'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, \n",
    "            'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7\n",
    "        }\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 6:  # Ensure we have enough columns\n",
    "                    # Convert day name to integer\n",
    "                    day_num = day_map.get(values[3], 0)\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # class_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        day_num,              # day_of_week as int\n",
    "                        values[4],            # time_start\n",
    "                        values[5]             # time_end\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted schedules successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load schedules: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def populate_cassandra_denormalized_tables(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"\n",
    "    Populate denormalized tables for efficient queries.\n",
    "    This requires more memory as we need to join data in Python.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # --- Populate student_enrollments table ---\n",
    "        # This requires joining enrollments with classes\n",
    "        enrollments_df = pd.read_csv(data_path / 'enrollments.csv')\n",
    "        classes_df = pd.read_csv(data_path / 'classes.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_enrollments (student_id, class_id, class_name, teacher_id, enrolled_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process\n",
    "        merged = pd.merge(enrollments_df, classes_df, left_on='class_id', right_on='id')\n",
    "        for _, row in merged.iterrows():\n",
    "            enrolled_at = datetime.fromisoformat(row['enrolled_at'].replace('Z', '+00:00'))\n",
    "            \n",
    "            session.execute(stmt, [\n",
    "                int(row['student_id']),\n",
    "                int(row['class_id']),\n",
    "                row['name'],\n",
    "                int(row['teacher_id']),\n",
    "                enrolled_at\n",
    "            ])\n",
    "        \n",
    "        print(\"INFO: Populated student_enrollments denormalized table\")\n",
    "        \n",
    "        # --- Populate student_grades table ---\n",
    "        # This requires joining grades with subjects\n",
    "        grades_df = pd.read_csv(data_path / 'grades.csv')\n",
    "        subjects_df = pd.read_csv(data_path / 'subjects.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_grades (student_id, subject_id, subject_name, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process\n",
    "        merged = pd.merge(grades_df, subjects_df, left_on='subject_id', right_on='id')\n",
    "        for _, row in merged.iterrows():\n",
    "            created_at = datetime.fromisoformat(row['created_at_x'].replace('Z', '+00:00'))\n",
    "            \n",
    "            session.execute(stmt, [\n",
    "                int(row['student_id']),\n",
    "                int(row['subject_id']),\n",
    "                row['name'],\n",
    "                float(row['grade']),\n",
    "                created_at\n",
    "            ])\n",
    "        \n",
    "        print(\"INFO: Populated student_grades denormalized table\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to populate denormalized tables: {e}\")\n",
    "\n",
    "def load_cassandra_data(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"Load all data into Cassandra tables.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Use the keyspace\n",
    "    session.execute(f\"USE {keyspace};\")\n",
    "    \n",
    "    # Insert basic entities\n",
    "    op_time, f_op_time, end_time = insert_cassandra_teachers(session, data_path / 'teachers.csv')\n",
    "    print(f\"INFO: Inserted teachers in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_subjects(session, data_path / 'subjects.csv')\n",
    "    print(f\"INFO: Inserted subjects in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_classes(session, data_path / 'classes.csv')\n",
    "    print(f\"INFO: Inserted classes in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_students(session, data_path / 'students.csv')\n",
    "    print(f\"INFO: Inserted students in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Insert relationships and complex data\n",
    "    op_time, f_op_time, end_time = insert_cassandra_enrollments(session, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_grades(session, data_path / 'grades.csv')\n",
    "    print(f\"INFO: Inserted grades in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_schedules(session, data_path / 'schedules.csv')\n",
    "    print(f\"INFO: Inserted schedules in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Populate denormalized tables\n",
    "    populate_cassandra_denormalized_tables(session, data_path)\n",
    "\n",
    "def verify_cassandra_counts(session, keyspace='benchmark'):\n",
    "    \"\"\"Count rows in all Cassandra tables.\"\"\"\n",
    "    tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                'enrollments', 'grades', 'schedules', \n",
    "                'student_enrollments', 'student_grades']\n",
    "    max_len = max(len(t) for t in tables)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        counts = {}\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                rows = session.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                count = rows.one()[0]\n",
    "                counts[table] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[table] = 'Error'\n",
    "        \n",
    "        print(\"--- Cassandra Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"----------------------------------\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa6f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dropped Cassandra table: teachers\n",
      "INFO: Dropped Cassandra table: subjects\n",
      "INFO: Dropped Cassandra table: classes\n",
      "INFO: Dropped Cassandra table: students\n",
      "INFO: Dropped Cassandra table: enrollments\n",
      "INFO: Dropped Cassandra table: grades\n",
      "INFO: Dropped Cassandra table: schedules\n",
      "INFO: Dropped Cassandra table: student_enrollments\n",
      "INFO: Dropped Cassandra table: student_grades\n",
      "INFO: Cassandra schema initialized.\n",
      "INFO: All Cassandra tables exist: teachers, subjects, classes, students, enrollments, grades, schedules\n",
      "INFO: Inserted teachers successfully\n",
      "INFO: Inserted teachers in 0.19 seconds (file opened in 0.18 seconds)\n",
      "INFO: Inserted subjects successfully\n",
      "INFO: Inserted subjects in 0.24 seconds (file opened in 0.23 seconds)\n",
      "INFO: Inserted classes successfully\n",
      "INFO: Inserted classes in 0.53 seconds (file opened in 0.52 seconds)\n",
      "INFO: Inserted students successfully\n",
      "INFO: Inserted students in 2.58 seconds (file opened in 2.57 seconds)\n",
      "INFO: Inserted enrollments successfully\n",
      "INFO: Inserted enrollments in 4.99 seconds (file opened in 4.98 seconds)\n",
      "INFO: Inserted grades successfully\n",
      "INFO: Inserted grades in 7.90 seconds (file opened in 7.89 seconds)\n",
      "INFO: Inserted schedules successfully\n",
      "INFO: Inserted schedules in 1.21 seconds (file opened in 1.20 seconds)\n",
      "INFO: Populated student_enrollments denormalized table\n",
      "INFO: Populated student_grades denormalized table\n",
      "--- Cassandra Table Row Counts ---\n",
      "teachers            : 100\n",
      "subjects            : 100\n",
      "classes             : 200\n",
      "students            : 1000\n",
      "enrollments         : 1994\n",
      "grades              : 3000\n",
      "schedules           : 500\n",
      "student_enrollments : 1994\n",
      "student_grades      : 3000\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cassandra Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_cassandra_schema(cassandra_session)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_cassandra_tables(cassandra_session, expected_tables=required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_cassandra_data(cassandra_session, scale_100_dir)\n",
    "\n",
    "# Row count verification\n",
    "verify_cassandra_counts(cassandra_session)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_query(conn, query, params=None, fetch_size=None):\n",
    "    \"\"\"\n",
    "    Execute a generic SELECT query on PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        conn: PostgreSQL connection object\n",
    "        query: SQL query string\n",
    "        params: Optional parameters for the query (list, tuple or dict)\n",
    "        fetch_size: Number of rows to fetch at once (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'data': results, 'time': execution_time, 'row_count': count}\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    row_count = 0\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, params or ())\n",
    "            \n",
    "            if fetch_size:\n",
    "                # Fetch in batches\n",
    "                while True:\n",
    "                    rows = cur.fetchmany(fetch_size)\n",
    "                    if not rows:\n",
    "                        break\n",
    "                    results.extend([dict(zip([col[0] for col in cur.description], row)) for row in rows])\n",
    "                    row_count += len(rows)\n",
    "            else:\n",
    "                # Fetch all at once\n",
    "                rows = cur.fetchall()\n",
    "                results = [dict(zip([col[0] for col in cur.description], row)) for row in rows]\n",
    "                row_count = len(results)\n",
    "                \n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            'data': results,\n",
    "            'time': end_time - start_time,\n",
    "            'row_count': row_count\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: PostgreSQL query failed: {e}\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'time': time.time() - start_time,\n",
    "            'row_count': 0,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4b3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mariadb_query(conn, query, params=None, fetch_size=None):\n",
    "    \"\"\"\n",
    "    Execute a generic SELECT query on MariaDB.\n",
    "    \n",
    "    Args:\n",
    "        conn: MariaDB connection object\n",
    "        query: SQL query string\n",
    "        params: Optional parameters for the query (list, tuple or dict)\n",
    "        fetch_size: Number of rows to fetch at once (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'data': results, 'time': execution_time, 'row_count': count}\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    row_count = 0\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor(dictionary=True) as cur:\n",
    "            cur.execute(query, params or ())\n",
    "            \n",
    "            if fetch_size:\n",
    "                # Fetch in batches\n",
    "                while True:\n",
    "                    rows = cur.fetchmany(fetch_size)\n",
    "                    if not rows:\n",
    "                        break\n",
    "                    results.extend(rows)\n",
    "                    row_count += len(rows)\n",
    "            else:\n",
    "                # Fetch all at once\n",
    "                results = cur.fetchall()\n",
    "                row_count = len(results)\n",
    "                \n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            'data': results,\n",
    "            'time': end_time - start_time,\n",
    "            'row_count': row_count\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: MariaDB query failed: {e}\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'time': time.time() - start_time,\n",
    "            'row_count': 0,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "542996fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_query(client, collection_name, query=None, projection=None, sort=None, limit=None, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Execute a query (equivalent to SELECT) on MongoDB.\n",
    "    \n",
    "    Args:\n",
    "        client: MongoDB client object\n",
    "        collection_name: Name of the collection to query\n",
    "        query: MongoDB query dictionary (equivalent to WHERE)\n",
    "        projection: Fields to include/exclude (equivalent to SELECT columns)\n",
    "        sort: Sorting specification (equivalent to ORDER BY)\n",
    "        limit: Maximum number of documents to return (equivalent to LIMIT)\n",
    "        db_name: Database name\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'data': results, 'time': execution_time, 'row_count': count}\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        # Build the query\n",
    "        cursor = collection.find(\n",
    "            filter=query or {},\n",
    "            projection=projection\n",
    "        )\n",
    "        \n",
    "        # Apply sorting if specified\n",
    "        if sort:\n",
    "            cursor = cursor.sort(sort)\n",
    "            \n",
    "        # Apply limit if specified\n",
    "        if limit:\n",
    "            cursor = cursor.limit(limit)\n",
    "        \n",
    "        # Convert cursor to list of documents\n",
    "        results = list(cursor)\n",
    "        \n",
    "        # Convert ObjectId to string for JSON serialization\n",
    "        for doc in results:\n",
    "            if '_id' in doc and not isinstance(doc['_id'], (int, str, float)):\n",
    "                doc['_id'] = str(doc['_id'])\n",
    "        \n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            'data': results,\n",
    "            'time': end_time - start_time,\n",
    "            'row_count': len(results)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: MongoDB query failed: {e}\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'time': time.time() - start_time,\n",
    "            'row_count': 0,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_query(session, query, params=None, fetch_size=None, keyspace='benchmark'):\n",
    "    \"\"\"\n",
    "    Execute a CQL SELECT query on Cassandra.\n",
    "    \n",
    "    Args:\n",
    "        session: Cassandra session object\n",
    "        query: CQL query string\n",
    "        params: Optional parameters for the query (list, tuple or dict)\n",
    "        fetch_size: Number of rows to fetch at once (None for default)\n",
    "        keyspace: Keyspace name\n",
    "        \n",
    "    Returns:\n",
    "        dict: {'data': results, 'time': execution_time, 'row_count': count}\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Set the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Set fetch size if specified\n",
    "        if fetch_size:\n",
    "            session.default_fetch_size = fetch_size\n",
    "            \n",
    "        # Execute query\n",
    "        if params:\n",
    "            rows = session.execute(query, params)\n",
    "        else:\n",
    "            rows = session.execute(query)\n",
    "            \n",
    "        # Convert to list of dicts\n",
    "        results = []\n",
    "        for row in rows:\n",
    "            # Convert Row to dict and handle any non-serializable types\n",
    "            row_dict = {}\n",
    "            for key in row._fields:\n",
    "                value = getattr(row, key)\n",
    "                # Convert non-serializable types\n",
    "                if isinstance(value, (datetime.datetime, datetime.date)):\n",
    "                    value = value.isoformat()\n",
    "                elif isinstance(value, uuid.UUID):\n",
    "                    value = str(value)\n",
    "                row_dict[key] = value\n",
    "            results.append(row_dict)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        return {\n",
    "            'data': results,\n",
    "            'time': end_time - start_time,\n",
    "            'row_count': len(results)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cassandra query failed: {e}\")\n",
    "        return {\n",
    "            'data': [],\n",
    "            'time': time.time() - start_time,\n",
    "            'row_count': 0,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe916dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import statistics\n",
    "import contextlib\n",
    "import functools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Callable, Optional, Union\n",
    "from datetime import datetime\n",
    "\n",
    "class DatabaseBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive benchmarking wrapper for database operations that measures:\n",
    "    - Total execution time\n",
    "    - Individual operation times\n",
    "    - Throughput (ops/sec)\n",
    "    - System resource usage (CPU, RAM, I/O)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, benchmark_name: str, db_type: str, scale: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize benchmark settings\n",
    "        \n",
    "        Args:\n",
    "            benchmark_name: Name of the benchmark scenario\n",
    "            db_type: Type of database (postgres, mariadb, mongodb, cassandra)\n",
    "            scale: Data scale factor (100, 1000, etc.)\n",
    "        \"\"\"\n",
    "        self.benchmark_name = benchmark_name\n",
    "        self.db_type = db_type\n",
    "        self.scale = scale\n",
    "        self.results = {\n",
    "            'operations': [],\n",
    "            'durations': [],\n",
    "            'timestamps': [],\n",
    "            'cpu_samples': [],\n",
    "            'memory_samples': [],\n",
    "            'disk_read_samples': [],\n",
    "            'disk_write_samples': [],\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "        self.process = psutil.Process()\n",
    "        self.io_counters_start = None\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start the benchmark and initialize resource tracking\"\"\"\n",
    "        self.results['start_time'] = time.time()\n",
    "        self.io_counters_start = psutil.disk_io_counters()\n",
    "        return self\n",
    "    \n",
    "    def end(self):\n",
    "        \"\"\"End the benchmark and calculate final statistics\"\"\"\n",
    "        self.results['end_time'] = time.time()\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        total_time = self.results['end_time'] - self.results['start_time']\n",
    "        op_count = len(self.results['operations'])\n",
    "        \n",
    "        if op_count > 0:\n",
    "            avg_time = statistics.mean(self.results['durations'])\n",
    "            throughput = op_count / total_time\n",
    "            \n",
    "            # Calculate resource usage statistics\n",
    "            cpu_avg = statistics.mean(self.results['cpu_samples']) if self.results['cpu_samples'] else 0\n",
    "            memory_avg = statistics.mean(self.results['memory_samples']) / (1024 * 1024) if self.results['memory_samples'] else 0  # MB\n",
    "            \n",
    "            # Disk I/O statistics\n",
    "            io_counters_end = psutil.disk_io_counters()\n",
    "            read_bytes = io_counters_end.read_bytes - self.io_counters_start.read_bytes\n",
    "            write_bytes = io_counters_end.write_bytes - self.io_counters_start.write_bytes\n",
    "            \n",
    "            # Store summary statistics\n",
    "            self.summary = {\n",
    "                'benchmark_name': self.benchmark_name,\n",
    "                'db_type': self.db_type,\n",
    "                'scale': self.scale,\n",
    "                'total_time': total_time,\n",
    "                'operation_count': op_count,\n",
    "                'avg_operation_time': avg_time,\n",
    "                'throughput': throughput,\n",
    "                'cpu_avg': cpu_avg,\n",
    "                'memory_avg_mb': memory_avg,\n",
    "                'disk_read_mb': read_bytes / (1024 * 1024),\n",
    "                'disk_write_mb': write_bytes / (1024 * 1024),\n",
    "            }\n",
    "        else:\n",
    "            self.summary = {\n",
    "                'benchmark_name': self.benchmark_name,\n",
    "                'db_type': self.db_type,\n",
    "                'scale': self.scale,\n",
    "                'total_time': total_time,\n",
    "                'operation_count': 0,\n",
    "                'avg_operation_time': 0,\n",
    "                'throughput': 0\n",
    "            }\n",
    "        \n",
    "        return self.summary\n",
    "    \n",
    "    def record_operation(self, name: str, duration: float):\n",
    "        \"\"\"Record a single database operation with its duration\"\"\"\n",
    "        timestamp = time.time()\n",
    "        self.results['operations'].append(name)\n",
    "        self.results['durations'].append(duration)\n",
    "        self.results['timestamps'].append(timestamp)\n",
    "        \n",
    "        # Sample resource usage\n",
    "        self.results['cpu_samples'].append(self.process.cpu_percent())\n",
    "        self.results['memory_samples'].append(self.process.memory_info().rss)\n",
    "        \n",
    "        # Return the duration for convenience\n",
    "        return duration\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def measure_operation(self, name: str):\n",
    "        \"\"\"Context manager to measure a database operation\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            duration = time.time() - start_time\n",
    "            self.record_operation(name, duration)\n",
    "    \n",
    "    def wrap_function(self, func: Callable, operation_name: str = None):\n",
    "        \"\"\"\n",
    "        Decorator to wrap a function for benchmarking\n",
    "        \"\"\"\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            op_name = operation_name or func.__name__\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            duration = time.time() - start_time\n",
    "            self.record_operation(op_name, duration)\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    def generate_report(self, output_file: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Generate a detailed report of the benchmark results\"\"\"\n",
    "        if not hasattr(self, 'summary'):\n",
    "            self.end()  # Ensure summary is calculated\n",
    "            \n",
    "        # Create DataFrame for detailed operation timings\n",
    "        df = pd.DataFrame({\n",
    "            'operation': self.results['operations'],\n",
    "            'duration': self.results['durations'],\n",
    "            'timestamp': self.results['timestamps'],\n",
    "        })\n",
    "        \n",
    "        # Add relative timestamps\n",
    "        if len(df) > 0:\n",
    "            df['relative_time'] = df['timestamp'] - self.results['start_time']\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"BENCHMARK SUMMARY: {self.benchmark_name} - {self.db_type} (Scale: {self.scale})\")\n",
    "        print(f\"{'-' * 80}\")\n",
    "        print(f\"Total execution time:      {self.summary['total_time']:.4f} seconds\")\n",
    "        print(f\"Number of operations:      {self.summary['operation_count']} operations\")\n",
    "        print(f\"Average operation time:    {self.summary['avg_operation_time']:.4f} seconds\")\n",
    "        print(f\"Throughput:                {self.summary['throughput']:.2f} operations/second\")\n",
    "        print(f\"CPU usage (average):       {self.summary['cpu_avg']:.2f}%\")\n",
    "        print(f\"Memory usage (average):    {self.summary['memory_avg_mb']:.2f} MB\")\n",
    "        print(f\"Disk read:                 {self.summary['disk_read_mb']:.2f} MB\")\n",
    "        print(f\"Disk write:                {self.summary['disk_write_mb']:.2f} MB\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "        \n",
    "        # Save to file if requested\n",
    "        if output_file:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"{output_file}_{self.db_type}_{self.scale}_{timestamp}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Detailed results saved to: {filename}\")\n",
    "            \n",
    "            # Also save summary\n",
    "            summary_df = pd.DataFrame([self.summary])\n",
    "            summary_filename = f\"{output_file}_{self.db_type}_{self.scale}_{timestamp}_summary.csv\"\n",
    "            summary_df.to_csv(summary_filename, index=False)\n",
    "            print(f\"Summary saved to: {summary_filename}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Example usage functions for each database type\n",
    "def run_benchmark_scenario(db_type, scale, operations):\n",
    "    \"\"\"\n",
    "    Run a complete benchmark scenario, measuring all operations\n",
    "    \n",
    "    Args:\n",
    "        db_type: Database type (postgres, mariadb, mongodb, cassandra)\n",
    "        scale: Data scale factor\n",
    "        operations: List of operation functions to run\n",
    "        \n",
    "    Returns:\n",
    "        DatabaseBenchmark object with results\n",
    "    \"\"\"\n",
    "    benchmark = DatabaseBenchmark(f\"Complete-Scenario\", db_type, scale).start()\n",
    "    \n",
    "    try:\n",
    "        for op_name, op_func in operations:\n",
    "            with benchmark.measure_operation(op_name):\n",
    "                op_func()\n",
    "    finally:\n",
    "        benchmark.end()\n",
    "        benchmark.generate_report()\n",
    "    \n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY: Complete-Scenario - postgres (Scale: 100)\n",
      "--------------------------------------------------------------------------------\n",
      "Total execution time:      1.4843 seconds\n",
      "Number of operations:      10000 operations\n",
      "Average operation time:    0.0001 seconds\n",
      "Throughput:                6737.04 operations/second\n",
      "CPU usage (average):       22.30%\n",
      "Memory usage (average):    151.26 MB\n",
      "Disk read:                 1.88 MB\n",
      "Disk write:                0.48 MB\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DatabaseBenchmark at 0x108009cf0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def postgres_benchmark_example(postgres_client, student_id=1):\n",
    "    \"\"\"Run a sample benchmark for PostgreSQL operations\"\"\"\n",
    "    \n",
    "    # Define operations to benchmark\n",
    "    def op1():\n",
    "        # Simple student query\n",
    "        return postgres_query(\n",
    "            postgres_client,\n",
    "            \"SELECT * FROM students WHERE id = %s\",\n",
    "            [student_id]\n",
    "        )\n",
    "        \n",
    "    # Create operations list\n",
    "    operations = [(\"Student Query\", op1) for i in range(10000)]\n",
    "    \n",
    "    \n",
    "    # Run benchmark\n",
    "    return run_benchmark_scenario(\"postgres\", 100, operations)\n",
    "        \n",
    "postgres_benchmark_example(postgres_client, student_id=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
