{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae7c2b-1b56-4aa9-a2bf-d5c1748d744d",
   "metadata": {},
   "source": [
    "# Scenariusze testowe dla porównania wydajności baz danych\n",
    "\n",
    "### 1. Operacja CREATE\n",
    "\n",
    "- Dodanie nowego nauczyciela\n",
    "- Utworzenie nowej klasy\n",
    "- Dodanie nowego przedmiotu\n",
    "- Zarejestrowanie nowego ucznia\n",
    "- Przypisanie ucznia do klasy (**Dodano: Zapisanie ucznia do klasy (enrolment)**)\n",
    "- Utworzenie harmonogramu zajęć\n",
    "- Wystawienie oceny\n",
    "\n",
    "### 2. Operacja READ\n",
    "\n",
    "Pobranie kompleksowego raportu zawierającego:\n",
    "- Dane osobowe ucznia\n",
    "- Informacje o klasie (**Dodano: Informacje o zapisach do klas**)\n",
    "- Dane nauczyciela prowadzącego\n",
    "- Listę ocen z opisami przedmiotów\n",
    "- Szczegółowy harmonogram zajęć\n",
    "\n",
    "### 3. Operacja UPDATE\n",
    "\n",
    "- Aktualizacja danych ucznia\n",
    "- Zmiana przypisania do klasy (**Dodano: Aktualizacja zapisu do klasy**)\n",
    "- Modyfikacja nazwy klasy\n",
    "- Aktualizacja danych nauczyciela\n",
    "- Zmiana oceny\n",
    "- Aktualizacja opisu przedmiotu\n",
    "- Modyfikacja harmonogramu zajęć\n",
    "\n",
    "### 4. Operacja DELETE\n",
    "\n",
    "- Usunięcie ocen ucznia\n",
    "- Wypisanie ucznia z klasy (**Dodano: Usunięcie zapisu do klasy**)\n",
    "- Usunięcie harmonogramu zajęć\n",
    "- Usunięcie klasy\n",
    "- Opcjonalne usunięcie przedmiotów\n",
    "- Opcjonalne usunięcie nauczyciela\n",
    "- Usunięcie rekordu ucznia\n",
    "\n",
    "## Ilość rekordów do testów\n",
    "\n",
    "Testy będą przeprowadzane dla następujących ilości rekordów:\n",
    "\n",
    "1. 10,000 rekordów\n",
    "2. 100,000 rekordów\n",
    "3. 1,000,000 rekordów\n",
    "4. 10,000,000 rekordów\n",
    "\n",
    "## Metryki wydajnościowe\n",
    "\n",
    "Dla każdego scenariusza i ilości rekordów będziemy mierzyć:\n",
    "\n",
    "1. Czas wykonania całego scenariusza\n",
    "2. Średni czas pojedynczych operacji\n",
    "3. Liczbę operacji na sekundę (throughput)\n",
    "4. Zużycie zasobów systemowych (CPU, RAM, I/O dysku)\n",
    "\n",
    "# Narzędzia i technologie testowe\n",
    "\n",
    "### Wbudowane instrumenty bazodanowe\n",
    "\n",
    "Każdy system oferuje specjalizowane narzędzia diagnostyczne:\n",
    "\n",
    "| System | Narzędzie | Funkcjonalności |\n",
    "| :-- | :-- | :-- |\n",
    "| PostgreSQL | pgBench | Testy TPC-B, własne skrypty SQL |\n",
    "| MariaDB | sysbench | Testy OLTP, skalowanie pionowe |\n",
    "| MongoDB | mongoperf | Operacje na dokumentach JSON |\n",
    "| Cassandra | cassandra-stress | Testy dystrybucji danych |\n",
    "| Redis | redis-benchmark | Pomiar opóźnień operacji klucz-wartość |\n",
    "\n",
    "Wykorzystanie natywnych narzędzi pozwala na precyzyjne badanie specyficznych mechanizmów storage engine.\n",
    "\n",
    "### Automatyzacja w Pythonie\n",
    "\n",
    "Kluczowe biblioteki wspierające testy:\n",
    "\n",
    "- **SQLAlchemy** dla baz relacyjnych\n",
    "- **PyMongo** dla MongoDB\n",
    "- **Cassandra-driver** dla Cassandra\n",
    "- **redis-py** dla Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec37216-90ff-435c-9669-67af3f25c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import psycopg2\n",
    "import psycopg2.errors\n",
    "from pymongo import MongoClient\n",
    "from cassandra.cluster import Cluster\n",
    "import redis\n",
    "import mysql.connector\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Load database configuration\n",
    "print(\"Setting up database connections...\")\n",
    "with open('docker-compose.yml', 'r') as file:\n",
    "    docker_config = yaml.safe_load(file)\n",
    "\n",
    "# PostgreSQL connection\n",
    "postgres_config = docker_config['services']['postgresql']\n",
    "postgres_client = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    database=postgres_config['environment']['POSTGRES_DB'],\n",
    "    user=postgres_config['environment']['POSTGRES_USER'],\n",
    "    password=postgres_config['environment']['POSTGRES_PASSWORD'],\n",
    "    port=postgres_config['ports'][0].split(':')[0]\n",
    ")\n",
    "\n",
    "# MariaDB connection\n",
    "mariadb_config = docker_config['services']['mariadb']\n",
    "mariadb_client = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    database=mariadb_config['environment']['MYSQL_DATABASE'],\n",
    "    user=mariadb_config['environment']['MYSQL_USER'],\n",
    "    password=mariadb_config['environment']['MYSQL_PASSWORD'],\n",
    "    port=mariadb_config['ports'][0].split(':')[0],\n",
    "    allow_local_infile=True\n",
    ")\n",
    "\n",
    "# MongoDB connection\n",
    "mongo_config = docker_config['services']['mongodb']\n",
    "mongo_client = MongoClient(\n",
    "    host='localhost',\n",
    "    port=int(mongo_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Cassandra connection\n",
    "cassandra_config = docker_config['services']['cassandra']\n",
    "cassandra_client = Cluster(['localhost'], port=cassandra_config['ports'][0].split(':')[0])\n",
    "cassandra_session = cassandra_client.connect()\n",
    "\n",
    "# Redis connection\n",
    "redis_config = docker_config['services']['redis']\n",
    "redis_client = redis.Redis(\n",
    "    host='localhost',\n",
    "    port=int(redis_config['ports'][0].split(':')[0])\n",
    ")\n",
    "\n",
    "# Test connections\n",
    "try:\n",
    "    postgres_client.cursor().execute(\"SELECT 1\")\n",
    "    print(\"INFO: PostgreSQL connection successful\")\n",
    "    \n",
    "    mariadb_client.cursor(buffered=True).execute(\"SELECT 1\")\n",
    "    print(\"INFO: MariaDB connection successful\")\n",
    "    \n",
    "    cassandra_session.execute(\"SELECT release_version FROM system.local\")\n",
    "    print(\"INFO: Cassandra connection successful\")\n",
    "    \n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"INFO: MongoDB connection successful\")\n",
    "    \n",
    "    redis_client.ping()\n",
    "    print(\"INFO: Redis connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7675e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_END = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd51d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation functions\n",
    "sys.path.append(str(Path.cwd()))\n",
    "from generator import generate_school_data\n",
    "\n",
    "def generate_files(output_dir='./data', scale=1000, batch_size=10000, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate synthetic school data files for benchmarking.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"INFO: Generating data with scale {scale} and batch size {batch_size}...\")\n",
    "\n",
    "    result = generate_school_data(\n",
    "        output_dir=output_dir,\n",
    "        scale=scale,\n",
    "        batch_size=batch_size,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"INFO: Generated {len(result['students'])} students, {len(result['teachers'])} teachers, \" + \n",
    "        f\"{len(result['classes'])} classes, {len(result['subjects'])} subjects\")\n",
    "    print(\"=\"*50)\n",
    "    return result\n",
    "\n",
    "# Generate test data sets\n",
    "scale_100_dir = './data/scale_100'\n",
    "\n",
    "generate_files(output_dir=scale_100_dir, scale=100, batch_size=5000)\n",
    "CELL_END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2dd2d0",
   "metadata": {},
   "source": [
    "# PostgreSQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Methods\n",
    "\n",
    "def initialize_postgres_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the PostgreSQL database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "        print(\"INFO: PostgreSQL schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing PostgreSQL schema: {e}\")\n",
    "\n",
    "def verify_postgres_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in PostgreSQL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_name = ANY(%s);\n",
    "            \"\"\", (expected_tables,))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All PostgreSQL tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing PostgreSQL tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying PostgreSQL tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using INSERT.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # Skip header\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    insert_sql = f\"INSERT INTO {table_name} VALUES ({', '.join(['%s'] * len(values))})\"\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except psycopg2.errors.UniqueViolation:\n",
    "                        # Ignore duplicate key errors\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a PostgreSQL table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_postgres_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create a temporary table for the COPY operation\n",
    "            temp_table_name = \"temp_enrollments\"\n",
    "            cur.execute(f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table_name} (\n",
    "                    student_id INT,\n",
    "                    class_id INT,\n",
    "                    enrolled_at TIMESTAMP\n",
    "                ) ON COMMIT DROP;\n",
    "            \"\"\")\n",
    "            copy_sql = f\"COPY {temp_table_name} FROM STDIN WITH (FORMAT CSV, HEADER)\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time()\n",
    "                cur.copy_expert(sql=copy_sql, file=f)\n",
    "\n",
    "            # Insert into the main table, ignoring duplicates\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                SELECT student_id, class_id, enrolled_at FROM {temp_table_name}\n",
    "                ON CONFLICT (student_id, class_id) DO NOTHING;\n",
    "            \"\"\"\n",
    "            cur.execute(insert_sql)\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_postgres_data(conn, data_dir):\n",
    "    \"\"\"\n",
    "    Loads data from CSV files into PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    table_csv_map = {\n",
    "        'teachers': 'teachers.csv',\n",
    "        'subjects': 'subjects.csv',\n",
    "        'classes': 'classes.csv',\n",
    "        'students': 'students.csv',\n",
    "        'grades': 'grades.csv',\n",
    "        'schedules': 'schedules.csv',\n",
    "        # 'enrollments': 'enrollments.csv' # Handled separately\n",
    "    }\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = copy_postgres_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_postgres_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_postgres_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in PostgreSQL tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in PostgreSQL tables\")\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- PostgreSQL Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "with open('schemas/postgres_schema.sql', 'r') as f:\n",
    "    sql_schema = f.read()\n",
    "\n",
    "initialize_postgres_schema(postgres_client, sql_schema)\n",
    "\n",
    "# Table verification \n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_postgres_tables(postgres_client, required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_postgres_data(postgres_client, scale_100_dir)\n",
    "\n",
    "# Count verification\n",
    "verify_postgres_counts(postgres_client, required_tables)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721535d9",
   "metadata": {},
   "source": [
    "# MariaDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB Methods\n",
    "\n",
    "def initialize_mariadb_schema(conn, schema_sql):\n",
    "    \"\"\"\n",
    "    Initializes the MariaDB database schema using the provided SQL script.\n",
    "    \"\"\"\n",
    "    if not schema_sql:\n",
    "        print(\"ERROR: Schema SQL content is empty.\")\n",
    "        return\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for statement in schema_sql.split(';'):\n",
    "                stmt = statement.strip()\n",
    "                if stmt:\n",
    "                    cur.execute(stmt)\n",
    "        conn.commit()\n",
    "        print(\"INFO: MariaDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error initializing MariaDB schema: {e}\")\n",
    "\n",
    "def verify_mariadb_tables(conn, expected_tables):\n",
    "    \"\"\"\n",
    "    Verifies if the expected tables exist in MariaDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            format_strings = ','.join(['%s'] * len(expected_tables))\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = DATABASE() AND table_name IN ({format_strings});\n",
    "            \"\"\", tuple(expected_tables))\n",
    "            existing_tables = {row[0] for row in cur.fetchall()}\n",
    "\n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All MariaDB tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MariaDB tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error verifying MariaDB tables: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    \"\"\"Inserts data from a CSV file into a MariaDB table by reading the header for columns.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            with open(csv_file, 'r') as f:\n",
    "                # read header for column names\n",
    "                header = next(f).strip().split(',')\n",
    "                cols = header\n",
    "                placeholders = ','.join(['%s'] * len(cols))\n",
    "                insert_sql = f\"INSERT INTO {table_name} ({','.join(cols)}) VALUES ({placeholders})\"\n",
    "                file_opened_start_time = time.time()\n",
    "                for line in f:\n",
    "                    values = line.strip().split(',')\n",
    "                    # ensure values length matches columns\n",
    "                    if len(values) != len(cols):\n",
    "                        if len(values) > len(cols):\n",
    "                            values = values[:len(cols)]\n",
    "                        else:\n",
    "                            print(f\"WARNING: Skipping {table_name} row with {len(values)} values (expected {len(cols)}, values: {values})\")\n",
    "                            continue\n",
    "                    try:\n",
    "                        cur.execute(insert_sql, values)\n",
    "                    except mysql.connector.errors.IntegrityError:\n",
    "                        conn.rollback()\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: Error inserting into {table_name}: {e}\")\n",
    "                        conn.rollback()\n",
    "                        break\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "\n",
    "def copy_mariadb_table_from_csv(conn, table_name, csv_file) -> tuple[float, float, float]:\n",
    "    # Inserts data from a CSV file into a MariaDB table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Assumes the table already exists and has the same structure as the CSV file.\n",
    "    \n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            copy_sql = f\"\"\"\n",
    "            LOAD DATA LOCAL INFILE '{csv_file}'\n",
    "            INTO TABLE {table_name}\n",
    "            FIELDS TERMINATED BY ','\n",
    "            OPTIONALLY ENCLOSED BY '\"'\n",
    "            LINES TERMINATED BY '\\n'\n",
    "            IGNORE 1 LINES;\n",
    "            \"\"\"\n",
    "            with open(csv_file, 'r') as f:\n",
    "                file_opened_start_time = time.time() # Initialize start_time when file is opened\n",
    "                cur.execute(copy_sql)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into {table_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def copy_mariadb_enrollments_from_csv(conn, csv_file):\n",
    "    # This function is a specialized version for the enrollments table, because it has a composite primary key.\n",
    "    # Inserts data from a CSV file into the enrollments table using COPY.\n",
    "    # Does not fail on duplicate key errors.\n",
    "    # Uses a temporary table to handle duplicates.\n",
    "\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Handle enrollments with INSERT IGNORE to skip duplicates\n",
    "            print(f\"INFO: Loading enrollments with duplicate handling...\")\n",
    "            with open(csv_file, 'r') as f:\n",
    "                next(f)  # skip header\n",
    "                for line in f:\n",
    "                    student_id, class_id, enrolled_at = line.strip().split(',')\n",
    "                    cur.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT IGNORE INTO enrollments (student_id, class_id, enrolled_at)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        \"\"\",\n",
    "                        (student_id, class_id, enrolled_at)\n",
    "                    )\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"ERROR: Error inserting data from {csv_file} into enrollments: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def load_mariadb_data(conn, data_dir):\n",
    "    table_csv_map = {\n",
    "    'teachers': 'teachers.csv',\n",
    "    'subjects': 'subjects.csv',\n",
    "    'classes': 'classes.csv',\n",
    "    'students': 'students.csv',\n",
    "    'grades': 'grades.csv',\n",
    "    'schedules': 'schedules.csv',\n",
    "    # 'enrollments': 'enrollments.csv' Handled separately\n",
    "    }\n",
    "    data_path = Path(data_dir)\n",
    "    for table_name, csv_file in table_csv_map.items():\n",
    "        op_time, f_op_time, end_time = insert_mariadb_table_from_csv(conn, table_name, data_path / csv_file)\n",
    "        print(f\"INFO: Inserted {table_name} in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "    # Handle enrollments separately due to composite primary key\n",
    "    op_time, f_op_time, end_time = copy_mariadb_enrollments_from_csv(conn, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "\n",
    "def verify_mariadb_counts(conn, tables):\n",
    "    \"\"\"\n",
    "    Counts rows in MariaDB tables.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    max_len = max(len(t) for t in tables) if tables else 0\n",
    "    print(f\"INFO: Counting rows in MariaDB tables\")\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            for table_name in tables:\n",
    "                try:\n",
    "                    cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "                    count = cur.fetchone()[0]\n",
    "                    counts[table_name] = count\n",
    "                except Exception as count_error:\n",
    "                    print(f\"ERROR: {count_error}\")\n",
    "                    counts[table_name] = 'Error'\n",
    "\n",
    "        print(\"--- MariaDB Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"---------------------------------\")\n",
    "        return counts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MariaDB Operations Execution\n",
    "\n",
    "# # Schema initialization\n",
    "# with open('schemas/mariadb_schema.sql', 'r') as f:\n",
    "#     mariadb_schema = f.read()\n",
    "\n",
    "# initialize_mariadb_schema(mariadb_client, mariadb_schema)\n",
    "\n",
    "# # Table verification\n",
    "# required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "# verify_mariadb_tables(mariadb_client, required_tables)\n",
    "\n",
    "# # Data loading\n",
    "# load_mariadb_data(mariadb_client, scale_100_dir)\n",
    "\n",
    "# # Count verification\n",
    "# verify_mariadb_counts(mariadb_client, required_tables)\n",
    "# CELL_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eb7b3",
   "metadata": {},
   "source": [
    "# MongoDB Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Methods\n",
    "def initialize_mongo_schema(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Initializes the MongoDB schema by creating necessary collections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        \n",
    "        # List of collections to create based on no_sql_design.txt\n",
    "        collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "        \n",
    "        # Drop existing collections if they exist\n",
    "        for collection in collections:\n",
    "            if collection in db.list_collection_names():\n",
    "                db[collection].drop()\n",
    "                print(f\"INFO: Dropped MongoDB collection: {collection}\")\n",
    "        \n",
    "        # Create collections with indexes\n",
    "        for collection in collections:\n",
    "            db.create_collection(collection)\n",
    "            print(f\"INFO: Created MongoDB collection: {collection}\")\n",
    "            \n",
    "            # Create indexes for performance\n",
    "            if collection == 'students':\n",
    "                db[collection].create_index([(\"last_name\", 1), (\"first_name\", 1)])\n",
    "            elif collection == 'classes':\n",
    "                db[collection].create_index([(\"name\", 1)])\n",
    "                \n",
    "        print(\"INFO: MongoDB schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_mongo_collections(client, db_name='benchmark', expected_collections=None):\n",
    "    \"\"\"\n",
    "    Verifies if the expected collections exist in MongoDB.\n",
    "    \"\"\"\n",
    "    if expected_collections is None:\n",
    "        expected_collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        existing_collections = db.list_collection_names()\n",
    "        \n",
    "        missing_collections = set(expected_collections) - set(existing_collections)\n",
    "        if not missing_collections:\n",
    "            print(f\"INFO: All MongoDB collections exist: {', '.join(expected_collections)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing MongoDB collections: {', '.join(missing_collections)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_mongo_data_from_csv(client, collection_name, csv_file) -> tuple[float, float, float]:\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "    file_opened_start_time = 0 # Initialize file_opened_start_time\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db[collection_name]\n",
    "        \n",
    "        with open(csv_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            # rename id to _id for MongoDB\n",
    "            if 'id' in reader.columns:\n",
    "                reader.rename(columns={'id': '_id'}, inplace=True)\n",
    "\n",
    "            file_opened_start_time = time.time() # Initialize start_time just before starting to insert\n",
    "            for _, row in reader.iterrows():\n",
    "                doc = row.to_dict()\n",
    "                collection.insert_one(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_mongo_students_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all grades and enrollments into students from csv files\n",
    "    # create a student object with embedded enrollments and grades\n",
    "    students_file = data_path / 'students.csv'\n",
    "    enrollments_file = data_path / 'enrollments.csv'\n",
    "    grades_file = data_path / 'grades.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['students']\n",
    "        \n",
    "        with open(students_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"first_name\": row['first_name'],\n",
    "                    \"last_name\": row['last_name'],\n",
    "                    \"birth_date\": row['birth_date'],\n",
    "                    \"enrollments\": [],\n",
    "                    \"grades\": []\n",
    "                }\n",
    "                collection.insert_one(student_doc)\n",
    "\n",
    "        with open(enrollments_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                enrollment_doc = {\n",
    "                    \"class_id\": row['class_id'],\n",
    "                    \"enrolled_at\": row['enrolled_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"enrollments\": enrollment_doc}}\n",
    "                )\n",
    "\n",
    "        with open(grades_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                student_id = row['student_id']\n",
    "                grade_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"grade\": row['grade'],\n",
    "                    \"created_at\": row['created_at']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": student_id},\n",
    "                    {\"$push\": {\"grades\": grade_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "\n",
    "def insert_mongo_classes_from_csv(client, data_path) -> tuple[float, float]:\n",
    "    # load all teachers and schedules into classes from csv files\n",
    "    # create a class object with embedded teachers and schedules\n",
    "    classes_file = data_path / 'classes.csv'\n",
    "    schedules_file = data_path / 'schedules.csv'\n",
    "    operation_start_time = time.time() # Initialize start_time\n",
    "\n",
    "    try:\n",
    "        db = client['benchmark']\n",
    "        collection = db['classes']\n",
    "\n",
    "        with open(classes_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_doc = {\n",
    "                    \"_id\": row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"teacher_id\": row['teacher_id'],\n",
    "                    \"schedule\": []\n",
    "                }\n",
    "                collection.insert_one(class_doc)\n",
    "\n",
    "        with open(schedules_file, 'r') as f:\n",
    "            reader = pd.read_csv(f)\n",
    "            for _, row in reader.iterrows():\n",
    "                class_id = row['class_id']\n",
    "                schedule_doc = {\n",
    "                    \"subject_id\": row['subject_id'],\n",
    "                    \"day_of_week\": row['day_of_week'],\n",
    "                    \"time_start\": row['time_start'],\n",
    "                    \"time_end\": row['time_end']\n",
    "                }\n",
    "                collection.update_one(\n",
    "                    {\"_id\": class_id},\n",
    "                    {\"$push\": {\"schedule\": schedule_doc}}\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        return (operation_start_time, end_time)\n",
    "\n",
    "def load_mongo_data(client, data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    insert_mongo_data_from_csv(client, 'teachers', data_path / 'teachers.csv')\n",
    "    insert_mongo_data_from_csv(client, 'subjects', data_path / 'subjects.csv')\n",
    "    insert_mongo_students_from_csv(client, data_path)\n",
    "    insert_mongo_classes_from_csv(client, data_path)\n",
    "\n",
    "def verify_mongo_counts(client, db_name='benchmark'):\n",
    "    \"\"\"\n",
    "    Counts documents in MongoDB collections.\n",
    "    \"\"\"\n",
    "    collections = ['students', 'teachers', 'classes', 'subjects']\n",
    "    max_len = max(len(c) for c in collections)\n",
    "    \n",
    "    try:\n",
    "        db = client[db_name]\n",
    "        counts = {}\n",
    "        \n",
    "        for collection in collections:\n",
    "            try:\n",
    "                count = db[collection].count_documents({})\n",
    "                counts[collection] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[collection] = 'Error'\n",
    "                \n",
    "        print(\"--- MongoDB Collection Document Counts ---\")\n",
    "        for collection, count in counts.items():\n",
    "            print(f\"{collection:<{max_len}} : {count}\")\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "        # Additional checks for embedded documents\n",
    "        try:\n",
    "            students_with_enrollments = db.students.count_documents({\"enrollments\": {\"$exists\": True, \"$ne\": []}})\n",
    "            students_with_grades = db.students.count_documents({\"grades\": {\"$exists\": True, \"$ne\": []}})\n",
    "            classes_with_schedules = db.classes.count_documents({\"schedule\": {\"$exists\": True, \"$ne\": []}})\n",
    "            \n",
    "            print(\"\\n--- MongoDB Embedded Document Counts ---\")\n",
    "            print(f\"Students with enrollments : {students_with_enrollments}\")\n",
    "            print(f\"Students with grades      : {students_with_grades}\")\n",
    "            print(f\"Classes with schedules    : {classes_with_schedules}\")\n",
    "            print(\"-----------------------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fdc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_mongo_schema(mongo_client)\n",
    "\n",
    "# Collection verification\n",
    "verify_mongo_collections(mongo_client)\n",
    "\n",
    "# Data loading\n",
    "load_mongo_data(mongo_client, scale_100_dir)\n",
    "\n",
    "# Document count verification\n",
    "verify_mongo_counts(mongo_client)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eff04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra data loading functions with minimal memory usage\n",
    "\n",
    "def initialize_cassandra_schema(session, keyspace='benchmark'):\n",
    "    \"\"\"Initializes the Cassandra schema by creating necessary keyspace and tables.\"\"\"\n",
    "    try:\n",
    "        # Create keyspace if not exists\n",
    "        session.execute(f\"\"\"\n",
    "            CREATE KEYSPACE IF NOT EXISTS {keyspace} \n",
    "            WITH REPLICATION = {{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }};\n",
    "        \"\"\")\n",
    "        \n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Drop existing tables if they exist\n",
    "        tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                    'enrollments', 'grades', 'schedules', \n",
    "                    'student_enrollments', 'student_grades']\n",
    "        \n",
    "        for table in tables:\n",
    "            session.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "            print(f\"INFO: Dropped Cassandra table: {table}\")\n",
    "        \n",
    "        # Create tables with appropriate data types\n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE teachers (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE subjects (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                description TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE classes (\n",
    "                id INT PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                teacher_id INT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE students (\n",
    "                id INT PRIMARY KEY,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT,\n",
    "                birth_date TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE grades (\n",
    "                id INT PRIMARY KEY,\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE schedules (\n",
    "                id INT PRIMARY KEY,\n",
    "                class_id INT,\n",
    "                subject_id INT,\n",
    "                day_of_week INT,\n",
    "                time_start TEXT,\n",
    "                time_end TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_enrollments (\n",
    "                student_id INT,\n",
    "                class_id INT,\n",
    "                class_name TEXT,\n",
    "                teacher_id INT,\n",
    "                enrolled_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, class_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE student_grades (\n",
    "                student_id INT,\n",
    "                subject_id INT,\n",
    "                subject_name TEXT,\n",
    "                grade FLOAT,\n",
    "                created_at TIMESTAMP,\n",
    "                PRIMARY KEY (student_id, subject_id, created_at)\n",
    "            ) WITH CLUSTERING ORDER BY (subject_id ASC, created_at DESC);\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"INFO: Cassandra schema initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "def verify_cassandra_tables(session, keyspace='benchmark', expected_tables=None):\n",
    "    \"\"\"Verifies if the expected tables exist in Cassandra.\"\"\"\n",
    "    if expected_tables is None:\n",
    "        expected_tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                            'enrollments', 'grades', 'schedules']\n",
    "    \n",
    "    try:\n",
    "        # Get existing tables\n",
    "        query = f\"\"\"\n",
    "            SELECT table_name FROM system_schema.tables \n",
    "            WHERE keyspace_name = '{keyspace}';\n",
    "        \"\"\"\n",
    "        rows = session.execute(query)\n",
    "        existing_tables = {row.table_name for row in rows}\n",
    "        \n",
    "        missing_tables = set(expected_tables) - existing_tables\n",
    "        if not missing_tables:\n",
    "            print(f\"INFO: All Cassandra tables exist: {', '.join(expected_tables)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"WARNING: Missing Cassandra tables: {', '.join(missing_tables)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False\n",
    "\n",
    "def insert_cassandra_teachers(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert teacher data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO teachers (id, first_name, last_name) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2]             # last_name\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted teachers successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load teachers: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_subjects(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert subject data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO subjects (id, name, description) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        values[2]             # description\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted subjects successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load subjects: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_classes(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert class data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO classes (id, name, teacher_id) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # name\n",
    "                        int(values[2])        # teacher_id\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted classes successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load classes: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_students(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert student data from CSV, line by line.\"\"\"\n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO students (id, first_name, last_name, birth_date) VALUES (?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # id\n",
    "                        values[1],            # first_name\n",
    "                        values[2],            # last_name\n",
    "                        values[3]             # birth_date\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted students successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load students: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_enrollments(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert enrollment data from CSV, line by line, with timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES (?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 3:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    enrolled_at = datetime.fromisoformat(values[2].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),       # student_id\n",
    "                        int(values[1]),       # class_id\n",
    "                        enrolled_at           # enrolled_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted enrollments successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load enrollments: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_grades(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert grades data from CSV, line by line, with UUID and timestamp handling.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 4:  # Ensure we have enough columns\n",
    "                    # Convert timestamp string to datetime object\n",
    "                    created_at = datetime.fromisoformat(values[4].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # student_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        float(values[3]),     # grade\n",
    "                        created_at            # created_at as datetime\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted grades successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load grades: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def insert_cassandra_schedules(session, csv_file, keyspace='benchmark') -> tuple:\n",
    "    \"\"\"Insert schedule data from CSV, line by line, with UUID and day mapping.\"\"\"\n",
    "    from uuid import uuid4\n",
    "    \n",
    "    operation_start_time = time.time()\n",
    "    file_opened_start_time = 0\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Day name to integer mapping\n",
    "        day_map = {\n",
    "            'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, \n",
    "            'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7\n",
    "        }\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        prepared_stmt = session.prepare(\n",
    "            \"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES (?, ?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Process CSV file line by line\n",
    "        with open(csv_file, 'r') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "            file_opened_start_time = time.time()\n",
    "            \n",
    "            for line in f:\n",
    "                values = line.strip().split(',')\n",
    "                if len(values) >= 6:  # Ensure we have enough columns\n",
    "                    # Convert day name to integer\n",
    "                    day_num = day_map.get(values[3], 0)\n",
    "                    \n",
    "                    session.execute(prepared_stmt, [\n",
    "                        int(values[0]),              # id (generated UUID)\n",
    "                        int(values[1]),       # class_id\n",
    "                        int(values[2]),       # subject_id\n",
    "                        day_num,              # day_of_week as int\n",
    "                        values[4],            # time_start\n",
    "                        values[5]             # time_end\n",
    "                    ])\n",
    "        \n",
    "        print(\"INFO: Inserted schedules successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load schedules: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return (operation_start_time, file_opened_start_time, end_time)\n",
    "\n",
    "def populate_cassandra_denormalized_tables(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"\n",
    "    Populate denormalized tables for efficient queries.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # --- Populate student_enrollments table ---\n",
    "        print(\"INFO: Populating student_enrollments table...\")\n",
    "        enrollments_df = pd.read_csv(data_path / 'enrollments.csv')\n",
    "        classes_df = pd.read_csv(data_path / 'classes.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_enrollments (student_id, class_id, class_name, teacher_id, enrolled_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process\n",
    "        merged = pd.merge(enrollments_df, classes_df, left_on='class_id', right_on='id')\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(merged), batch_size):\n",
    "            batch = merged.iloc[i:i+batch_size]\n",
    "            for _, row in batch.iterrows():\n",
    "                try:\n",
    "                    enrolled_at = datetime.fromisoformat(row['enrolled_at'].replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(stmt, [\n",
    "                        int(row['student_id']),\n",
    "                        int(row['class_id']),\n",
    "                        str(row['name']),\n",
    "                        int(row['teacher_id']),\n",
    "                        enrolled_at\n",
    "                    ])\n",
    "                except Exception as e:\n",
    "                    print(f\"WARNING: Error inserting row into student_enrollments: {e}\")\n",
    "        \n",
    "        print(\"INFO: Populated student_enrollments denormalized table\")\n",
    "        \n",
    "        # --- Populate student_grades table ---\n",
    "        print(\"INFO: Populating student_grades table...\")\n",
    "        grades_df = pd.read_csv(data_path / 'grades.csv')\n",
    "        subjects_df = pd.read_csv(data_path / 'subjects.csv')\n",
    "        \n",
    "        # Prepare statement\n",
    "        stmt = session.prepare(\n",
    "            \"INSERT INTO student_grades (student_id, subject_id, subject_name, grade, created_at) VALUES (?, ?, ?, ?, ?)\"\n",
    "        )\n",
    "        \n",
    "        # Join and process - use explicit column references to avoid ambiguity\n",
    "        merged = pd.merge(grades_df, subjects_df, left_on='subject_id', right_on='id', suffixes=('_grade', '_subject'))\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        for i in range(0, len(merged), batch_size):\n",
    "            batch = merged.iloc[i:i+batch_size]\n",
    "            for _, row in batch.iterrows():\n",
    "                try:\n",
    "                    # Explicitly handle data types to avoid conversion errors and use the renamed column\n",
    "                    student_id = int(row['student_id'])\n",
    "                    subject_id = int(row['subject_id'])\n",
    "                    subject_name = str(row['name'])\n",
    "                    grade = float(row['grade'])\n",
    "                    created_at = datetime.fromisoformat(str(row['created_at_grade']).replace('Z', '+00:00'))\n",
    "                    \n",
    "                    session.execute(stmt, [\n",
    "                        student_id,\n",
    "                        subject_id,\n",
    "                        subject_name,\n",
    "                        grade,\n",
    "                        created_at\n",
    "                    ])\n",
    "                except Exception as e:\n",
    "                    # Add debug info to see column names\n",
    "                    print(f\"WARNING: Error inserting row into student_grades: {e}\")\n",
    "                    print(f\"DEBUG: Available columns: {list(row.index)}\")\n",
    "        \n",
    "        print(\"INFO: Populated student_grades denormalized table\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to populate denormalized tables: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def load_cassandra_data(session, data_dir, keyspace='benchmark'):\n",
    "    \"\"\"Load all data into Cassandra tables.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Use the keyspace\n",
    "    session.execute(f\"USE {keyspace};\")\n",
    "    \n",
    "    # Insert basic entities\n",
    "    op_time, f_op_time, end_time = insert_cassandra_teachers(session, data_path / 'teachers.csv')\n",
    "    print(f\"INFO: Inserted teachers in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_subjects(session, data_path / 'subjects.csv')\n",
    "    print(f\"INFO: Inserted subjects in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_classes(session, data_path / 'classes.csv')\n",
    "    print(f\"INFO: Inserted classes in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_students(session, data_path / 'students.csv')\n",
    "    print(f\"INFO: Inserted students in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Insert relationships and complex data\n",
    "    op_time, f_op_time, end_time = insert_cassandra_enrollments(session, data_path / 'enrollments.csv')\n",
    "    print(f\"INFO: Inserted enrollments in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_grades(session, data_path / 'grades.csv')\n",
    "    print(f\"INFO: Inserted grades in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    op_time, f_op_time, end_time = insert_cassandra_schedules(session, data_path / 'schedules.csv')\n",
    "    print(f\"INFO: Inserted schedules in {end_time - op_time:.2f} seconds (file opened in {end_time - f_op_time:.2f} seconds)\")\n",
    "    \n",
    "    # Populate denormalized tables\n",
    "    populate_cassandra_denormalized_tables(session, data_path)\n",
    "\n",
    "def verify_cassandra_counts(session, keyspace='benchmark'):\n",
    "    \"\"\"Count rows in all Cassandra tables.\"\"\"\n",
    "    tables = ['teachers', 'subjects', 'classes', 'students', \n",
    "                'enrollments', 'grades', 'schedules', \n",
    "                'student_enrollments', 'student_grades']\n",
    "    max_len = max(len(t) for t in tables)\n",
    "    \n",
    "    try:\n",
    "        # Use the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        counts = {}\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                rows = session.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                count = rows.one()[0]\n",
    "                counts[table] = count\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                counts[table] = 'Error'\n",
    "        \n",
    "        print(\"--- Cassandra Table Row Counts ---\")\n",
    "        for table, count in counts.items():\n",
    "            print(f\"{table:<{max_len}} : {count}\")\n",
    "        print(\"----------------------------------\")\n",
    "        \n",
    "        return counts\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra Operations Execution\n",
    "\n",
    "# Schema initialization\n",
    "initialize_cassandra_schema(cassandra_session)\n",
    "\n",
    "# Table verification\n",
    "required_tables = ['teachers', 'subjects', 'classes', 'students', 'enrollments', 'grades', 'schedules']\n",
    "verify_cassandra_tables(cassandra_session, expected_tables=required_tables)\n",
    "\n",
    "# Data loading\n",
    "load_cassandra_data(cassandra_session, scale_100_dir)\n",
    "\n",
    "# Row count verification\n",
    "verify_cassandra_counts(cassandra_session)\n",
    "CELL_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_operation(conn, query, fetch=False):\n",
    "    \"\"\"\n",
    "    Executes a PostgreSQL operation and returns the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            if fetch:\n",
    "                result = cur.fetchall()\n",
    "                print(result)\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        conn.rollback()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mariadb_operation(conn, query, fetch=False):\n",
    "    \"\"\"\n",
    "    Executes a MariaDB operation and returns the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            if fetch:\n",
    "                result = cur.fetchall()\n",
    "                print(result)\n",
    "        \n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        conn.rollback()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542996fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_operation(client, db_name, collection_name, operation_type, query=None, data=None, options=None, fetch=False):\n",
    "    \"\"\"\n",
    "    Executes a MongoDB operation and returns the result.\n",
    "    \n",
    "    Args:\n",
    "        client: MongoDB client connection\n",
    "        db_name: Database name to operate on\n",
    "        collection_name: Collection name to operate on\n",
    "        operation_type: Type of operation ('find', 'insert', 'update', 'delete')\n",
    "        query: Query filter for find/update/delete operations (dict)\n",
    "        data: Data for insert/update operations (dict or list of dicts)\n",
    "        options: Additional options for operations (dict)\n",
    "        fetch: Whether to fetch and print results (boolean)\n",
    "        \n",
    "    Returns:\n",
    "        Operation result or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get database and collection references\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        result = None\n",
    "        \n",
    "        # Execute the requested operation\n",
    "        if operation_type == 'find':\n",
    "            # For find operations\n",
    "            query = query or {}\n",
    "            options = options or {}\n",
    "            cursor = collection.find(query, **options)\n",
    "            result = list(cursor)\n",
    "            if fetch:\n",
    "                print(result)\n",
    "                \n",
    "        elif operation_type == 'insert':\n",
    "            # For insert operations\n",
    "            if isinstance(data, list):\n",
    "                result = collection.insert_many(data)\n",
    "            else:\n",
    "                result = collection.insert_one(data)\n",
    "                \n",
    "        elif operation_type == 'update':\n",
    "            # For update operations\n",
    "            options = options or {}\n",
    "            if options.get('multi', False):\n",
    "                result = collection.update_many(query, data, **options)\n",
    "            else:\n",
    "                result = collection.update_one(query, data, **options)\n",
    "                \n",
    "        elif operation_type == 'delete':\n",
    "            # For delete operations\n",
    "            options = options or {}\n",
    "            if options.get('multi', False):\n",
    "                result = collection.delete_many(query, **options)\n",
    "            else:\n",
    "                result = collection.delete_one(query, **options)\n",
    "                \n",
    "        else:\n",
    "            print(f\"ERROR: Unsupported operation type: {operation_type}\")\n",
    "            return None\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_operation(session, query, params=None, fetch=False, keyspace='benchmark'):\n",
    "    \"\"\"\n",
    "    Executes a Cassandra CQL operation and returns the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set the keyspace\n",
    "        session.execute(f\"USE {keyspace};\")\n",
    "        \n",
    "        # Execute the query with or without parameters\n",
    "        if params:\n",
    "            # For prepared statements, we need to prepare first\n",
    "            prepared = session.prepare(query)\n",
    "            result = session.execute(prepared, params)\n",
    "        else:\n",
    "            result = session.execute(query)\n",
    "        \n",
    "        # Rest of your function remains the same...\n",
    "        if fetch and query.strip().upper().startswith('SELECT'):\n",
    "            rows = list(result)\n",
    "            for row in rows:\n",
    "                print(row)\n",
    "            return rows\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cassandra operation failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb66048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id,first_name,last_name,subject,hire_date,created_at\n",
    "test_teachers = {\n",
    "    10000001: {\"first_name\": \"Anna\",    \"last_name\": \"Smith\",   \"subject\": \"Mathematics\",    \"hire_date\": \"2010-01-15\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000002: {\"first_name\": \"James\",   \"last_name\": \"Lee\",     \"subject\": \"History\",        \"hire_date\": \"2012-08-20\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000003: {\"first_name\": \"Maria\",   \"last_name\": \"Garcia\",  \"subject\": \"Biology\",        \"hire_date\": \"2015-03-10\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000004: {\"first_name\": \"David\",   \"last_name\": \"Johnson\", \"subject\": \"Chemistry\",      \"hire_date\": \"2011-11-01\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000005: {\"first_name\": \"Linda\",   \"last_name\": \"Brown\",   \"subject\": \"English\",        \"hire_date\": \"2013-04-22\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000006: {\"first_name\": \"Robert\",  \"last_name\": \"Jones\",   \"subject\": \"Physics\",        \"hire_date\": \"2014-09-30\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000007: {\"first_name\": \"Patricia\",\"last_name\": \"Miller\",  \"subject\": \"Art\",            \"hire_date\": \"2016-06-17\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000008: {\"first_name\": \"Michael\", \"last_name\": \"Davis\",   \"subject\": \"Geography\",      \"hire_date\": \"2009-02-05\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000009: {\"first_name\": \"Barbara\", \"last_name\": \"Wilson\",  \"subject\": \"Music\",          \"hire_date\": \"2017-12-12\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000010: {\"first_name\": \"William\", \"last_name\": \"Taylor\",  \"subject\": \"Computer Science\",\"hire_date\": \"2008-07-29\", \"created_at\": \"2025-05-05T12:00:00Z\"}\n",
    "}\n",
    "\n",
    "# id,name,description,created_at \n",
    "test_subjects = {\n",
    "    10000011: {\"name\": \"Mathematics\",        \"description\": \"Math fundamentals\",          \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000012: {\"name\": \"History\",            \"description\": \"World history overview\",    \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000013: {\"name\": \"Biology\",            \"description\": \"Life sciences\",             \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000014: {\"name\": \"Chemistry\",          \"description\": \"Chemical reactions\",        \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000015: {\"name\": \"English\",            \"description\": \"Literature and grammar\",    \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000016: {\"name\": \"Physics\",            \"description\": \"Mechanics and waves\",        \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000017: {\"name\": \"Art\",                \"description\": \"Art history and practice\",  \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000018: {\"name\": \"Geography\",          \"description\": \"Physical and human geo\",    \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000019: {\"name\": \"Music\",              \"description\": \"Theory and performance\",     \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000020: {\"name\": \"Computer Science\",   \"description\": \"Programming concepts\",      \"created_at\": \"2025-05-05T12:00:00Z\"}\n",
    "}\n",
    "\n",
    "# id,name,teacher_id,created_at\n",
    "test_classes = {\n",
    "    10000021: {\"name\": \"Algebra I\",      \"teacher_id\": 10000001, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000022: {\"name\": \"World History\",  \"teacher_id\": 10000002, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000023: {\"name\": \"Biology 101\",    \"teacher_id\": 10000003, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000024: {\"name\": \"Organic Chemistry\",\"teacher_id\":10000004, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000025: {\"name\": \"English Lit\",    \"teacher_id\": 10000005, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000026: {\"name\": \"Physics I\",      \"teacher_id\": 10000006, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000027: {\"name\": \"Drawing\",        \"teacher_id\": 10000007, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000028: {\"name\": \"World Geography\",\"teacher_id\": 10000008, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000029: {\"name\": \"Choir\",          \"teacher_id\": 10000009, \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000030: {\"name\": \"Intro to CS\",    \"teacher_id\": 10000010, \"created_at\": \"2025-05-05T12:00:00Z\"}\n",
    "}\n",
    "\n",
    "# id,first_name,last_name,birth_date,created_at\n",
    "test_students = {\n",
    "    10000031: {\"first_name\": \"John\",   \"last_name\": \"Doe\",    \"birth_date\": \"2005-06-15\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000032: {\"first_name\": \"Alice\",  \"last_name\": \"Wang\",   \"birth_date\": \"2006-11-02\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000033: {\"first_name\": \"Bob\",    \"last_name\": \"Nguyen\", \"birth_date\": \"2005-02-28\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000034: {\"first_name\": \"Carol\",  \"last_name\": \"Kim\",    \"birth_date\": \"2006-01-11\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000035: {\"first_name\": \"Eve\",    \"last_name\": \"Patel\",  \"birth_date\": \"2005-09-23\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000036: {\"first_name\": \"Frank\",  \"last_name\": \"Lopez\",  \"birth_date\": \"2006-07-05\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000037: {\"first_name\": \"Grace\",  \"last_name\": \"Chen\",   \"birth_date\": \"2005-12-19\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000038: {\"first_name\": \"Hank\",   \"last_name\": \"Singh\",  \"birth_date\": \"2006-03-30\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000039: {\"first_name\": \"Ivy\",    \"last_name\": \"Martinez\",\"birth_date\": \"2005-10-08\", \"created_at\": \"2025-05-05T12:00:00Z\"},\n",
    "    10000040: {\"first_name\": \"Jack\",   \"last_name\": \"Clark\",  \"birth_date\": \"2006-05-17\", \"created_at\": \"2025-05-05T12:00:00Z\"}\n",
    "}\n",
    "\n",
    "# student_id,class_id,enrolled_at\n",
    "test_enrollments = {\n",
    "    (10000031, 10000021): {\"enrolled_at\": \"2023-09-01T08:00:00Z\"},\n",
    "    (10000032, 10000022): {\"enrolled_at\": \"2023-09-02T09:00:00Z\"},\n",
    "    (10000033, 10000023): {\"enrolled_at\": \"2023-09-03T10:00:00Z\"},\n",
    "    (10000034, 10000024): {\"enrolled_at\": \"2023-09-04T11:00:00Z\"},\n",
    "    (10000035, 10000025): {\"enrolled_at\": \"2023-09-05T12:00:00Z\"},\n",
    "    (10000036, 10000026): {\"enrolled_at\": \"2023-09-06T13:00:00Z\"},\n",
    "    (10000037, 10000027): {\"enrolled_at\": \"2023-09-07T14:00:00Z\"},\n",
    "    (10000038, 10000028): {\"enrolled_at\": \"2023-09-08T15:00:00Z\"},\n",
    "    (10000039, 10000029): {\"enrolled_at\": \"2023-09-09T16:00:00Z\"},\n",
    "    (10000040, 10000030): {\"enrolled_at\": \"2023-09-10T17:00:00Z\"}\n",
    "}\n",
    "\n",
    "# id,class_id,subject_id,day_of_week,time_start,time_end\n",
    "test_schedules = {\n",
    "    10000051: {\"class_id\": 10000021, \"subject_id\": 10000011, \"day_of_week\": \"Monday\",    \"time_start\": \"08:00\", \"time_end\": \"09:30\"},\n",
    "    10000052: {\"class_id\": 10000022, \"subject_id\": 10000012, \"day_of_week\": \"Tuesday\",   \"time_start\": \"09:00\", \"time_end\": \"10:30\"},\n",
    "    10000053: {\"class_id\": 10000023, \"subject_id\": 10000013, \"day_of_week\": \"Wednesday\", \"time_start\": \"10:00\", \"time_end\": \"11:30\"},\n",
    "    10000054: {\"class_id\": 10000024, \"subject_id\": 10000014, \"day_of_week\": \"Thursday\",  \"time_start\": \"11:00\", \"time_end\": \"12:30\"},\n",
    "    10000055: {\"class_id\": 10000025, \"subject_id\": 10000015, \"day_of_week\": \"Friday\",    \"time_start\": \"12:00\", \"time_end\": \"13:30\"},\n",
    "    10000056: {\"class_id\": 10000026, \"subject_id\": 10000016, \"day_of_week\": \"Monday\",    \"time_start\": \"13:00\", \"time_end\": \"14:30\"},\n",
    "    10000057: {\"class_id\": 10000027, \"subject_id\": 10000017, \"day_of_week\": \"Tuesday\",   \"time_start\": \"14:00\", \"time_end\": \"15:30\"},\n",
    "    10000058: {\"class_id\": 10000028, \"subject_id\": 10000018, \"day_of_week\": \"Wednesday\", \"time_start\": \"15:00\", \"time_end\": \"16:30\"},\n",
    "    10000059: {\"class_id\": 10000029, \"subject_id\": 10000019, \"day_of_week\": \"Thursday\",  \"time_start\": \"16:00\", \"time_end\": \"17:30\"},\n",
    "    10000060: {\"class_id\": 10000030, \"subject_id\": 10000020, \"day_of_week\": \"Friday\",    \"time_start\": \"17:00\", \"time_end\": \"18:30\"}\n",
    "}\n",
    "\n",
    "# id,student_id,subject_id,grade,created_at\n",
    "test_grades = {\n",
    "    10000041: {\"student_id\": 10000031, \"subject_id\": 10000011, \"grade\": 85, \"created_at\": \"2024-05-10T12:00:00Z\"},\n",
    "    10000042: {\"student_id\": 10000032, \"subject_id\": 10000012, \"grade\": 92, \"created_at\": \"2024-06-15T14:30:00Z\"},\n",
    "    10000043: {\"student_id\": 10000033, \"subject_id\": 10000013, \"grade\": 78, \"created_at\": \"2024-07-20T16:45:00Z\"},\n",
    "    10000044: {\"student_id\": 10000034, \"subject_id\": 10000014, \"grade\": 88, \"created_at\": \"2024-08-22T10:15:00Z\"},\n",
    "    10000045: {\"student_id\": 10000035, \"subject_id\": 10000015, \"grade\": 91, \"created_at\": \"2024-09-05T09:00:00Z\"},\n",
    "    10000046: {\"student_id\": 10000036, \"subject_id\": 10000016, \"grade\": 79, \"created_at\": \"2024-10-12T11:20:00Z\"},\n",
    "    10000047: {\"student_id\": 10000037, \"subject_id\": 10000017, \"grade\": 94, \"created_at\": \"2024-11-30T13:50:00Z\"},\n",
    "    10000048: {\"student_id\": 10000038, \"subject_id\": 10000018, \"grade\": 82, \"created_at\": \"2024-12-18T15:05:00Z\"},\n",
    "    10000049: {\"student_id\": 10000039, \"subject_id\": 10000019, \"grade\": 76, \"created_at\": \"2025-01-25T08:40:00Z\"},\n",
    "    10000050: {\"student_id\": 10000040, \"subject_id\": 10000020, \"grade\": 89, \"created_at\": \"2025-02-14T14:10:00Z\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import statistics\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "class SimpleBenchmark:\n",
    "    \"\"\"Benchmark utility that prints results and returns pandas DataFrame for comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, db_type: str, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        self.db_type = db_type\n",
    "        self.process = psutil.Process()\n",
    "        self.results = []  # Store results for each scenario\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"\n",
    "        Convert the results to a pandas DataFrame for easier analysis and comparison.\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame containing all benchmark results\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def run_scenarios(self, scenarios: List[Tuple[str, List[Tuple[str, Callable]]]], \n",
    "                    setup_method: Callable = None, \n",
    "                    cleanup_method: Callable = None):\n",
    "        \"\"\"\n",
    "        Run multiple benchmark scenarios and collect metrics\n",
    "        \n",
    "        Args:\n",
    "            scenarios: List of (scenario_name, [(operation_name, function)]) tuples\n",
    "            setup_method: Optional function to run before each scenario (not measured)\n",
    "            cleanup_method: Optional function to run after each scenario (not measured)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: Results organized for comparison\n",
    "        \"\"\"\n",
    "        io_counters_start = psutil.disk_io_counters()\n",
    "        \n",
    "        # Run setup once if provided (not measured)\n",
    "        if setup_method:\n",
    "            setup_method()\n",
    "            \n",
    "        for scenario_name, operations in scenarios:\n",
    "            # Initialize metrics collection\n",
    "            start_time = time.time()\n",
    "            cpu_samples = []\n",
    "            memory_samples = []\n",
    "            durations = []\n",
    "            \n",
    "            # Execute all operations in the scenario\n",
    "            for op_name, func in operations:\n",
    "                # Sample CPU and memory\n",
    "                cpu_samples.append(self.process.cpu_percent())\n",
    "                memory_samples.append(self.process.memory_info().rss)\n",
    "                \n",
    "                # Execute function and measure time\n",
    "                op_start = time.time()\n",
    "                func()\n",
    "                op_duration = time.time() - op_start\n",
    "                durations.append(op_duration)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            # Resource usage\n",
    "            avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0\n",
    "            avg_memory = statistics.mean(memory_samples) / (1024 * 1024) if memory_samples else 0\n",
    "            \n",
    "            # Disk I/O \n",
    "            io_counters_end = psutil.disk_io_counters()\n",
    "            read_mb = (io_counters_end.read_bytes - io_counters_start.read_bytes) / (1024 * 1024)\n",
    "            write_mb = (io_counters_end.write_bytes - io_counters_start.write_bytes) / (1024 * 1024)\n",
    "            \n",
    "            # Performance metrics\n",
    "            avg_op_time = statistics.mean(durations) if durations else 0\n",
    "            throughput = len(operations) / total_time if total_time > 0 else 0\n",
    "            \n",
    "            # Save results for this scenario\n",
    "            scenario_result = {\n",
    "                'database': self.db_type,\n",
    "                'data_dir': self.data_dir,\n",
    "                'scenario': scenario_name,\n",
    "                'total_time': total_time,\n",
    "                'operations': len(operations),\n",
    "                'avg_operation_time': avg_op_time,\n",
    "                'throughput': throughput,\n",
    "                'cpu_avg': avg_cpu,\n",
    "                'memory_avg': avg_memory,\n",
    "                'disk_read_mb': read_mb,\n",
    "                'disk_write_mb': write_mb\n",
    "            }\n",
    "            self.results.append(scenario_result)\n",
    "            \n",
    "            # Print stats for this scenario\n",
    "            print(f\"--- {scenario_name} ({self.db_type}) ---\")\n",
    "            print(f\"Total time: {total_time:.4f} seconds\")\n",
    "            print(f\"Operations: {len(operations)}\")\n",
    "            print(f\"Avg operation time: {avg_op_time:.4f} seconds\")\n",
    "            print(f\"Throughput: {throughput:.2f} ops/sec\")\n",
    "            print(f\"CPU avg: {avg_cpu:.2f}%\")\n",
    "            print(f\"Memory avg: {avg_memory:.2f} MB\")\n",
    "            print(f\"Disk read: {read_mb:.2f} MB\")\n",
    "            print(f\"Disk write: {write_mb:.2f} MB\")\n",
    "            print()\n",
    "            \n",
    "            # Reset I/O counters for next scenario\n",
    "            io_counters_start = io_counters_end\n",
    "        \n",
    "        # Run cleanup if provided (not measured)\n",
    "        if cleanup_method:\n",
    "            cleanup_method()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652bf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_TEACHER=\"INSERT Teacher ?\"\n",
    "INSERT_SUBJECT=\"INSERT Subject ?\"\n",
    "INSERT_CLASS=\"INSERT Class ?\"\n",
    "INSERT_STUDENT=\"INSERT Student ?\"\n",
    "INSERT_ENROLLMENT=\"INSERT Enrollment ?\"\n",
    "INSERT_GRADE=\"INSERT Grade ?\"\n",
    "INSERT_SCHEDULE=\"INSERT Schedule ?\"\n",
    "\n",
    "SELECT_STUDENT = \"SELECT student ?\"\n",
    "SELECT_CLASS = \"SELECT class ?\"\n",
    "SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER = \"SELECT all students that are taught by the teacher ?\"\n",
    "SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT = \"SELECT all schedules for the student ?\"\n",
    "SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS = \"SELECT all grades with the subjects descriptions for all students in the class ?\"\n",
    "\n",
    "UPDATE_STUDENT_NAME = \"UPDATE Student ? Name\"\n",
    "UPDATE_ALL_GRADES_FOR_STUDENT = \"UPDATE all Grades for Student ?\"\n",
    "UPDATE_CLASS_NAME = \"UPDATE Class ? Name\"\n",
    "UPDATE_TEACHER_LAST_NAME = \"UPDATE Teacher ? Last Name\"\n",
    "UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM = \"UPDATE Subject ? Description for all subjects that students has grades from\"\n",
    "\n",
    "DELETE_STUDENT = \"DELETE Student ?\"\n",
    "DELETE_CLASS = \"DELETE Class ?\"\n",
    "DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER = \"DELETE Subjects that are in the schedule of the teacher ?\"\n",
    "DELETE_TEACHER_WHO_TAUGHT_STUDENT = \"DELETE Teacher who taught student ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e08098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgres_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = SimpleBenchmark(\"postgres\", data_dir)\n",
    "    def setup_for_insert():\n",
    "        with open('schemas/postgres_schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_postgres_schema(postgres_client, sql_schema)\n",
    "        load_postgres_data(postgres_client, data_dir)\n",
    "\n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TEACHER,\n",
    "            [\n",
    "                (INSERT_TEACHER + str(k), lambda k=k, v=v: postgres_operation(postgres_client, \n",
    "                    f\"INSERT INTO teachers (id, first_name, last_name, subject, hire_date, created_at) VALUES ({k}, '{v['first_name']}', '{v['last_name']}', '{v['subject']}', '{v['hire_date']}', '{v['created_at']}')\")) \n",
    "                for k, v in test_teachers.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SUBJECT,\n",
    "            [\n",
    "                (INSERT_SUBJECT + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO subjects (id, name, description, created_at) VALUES ({k}, '{v['name']}', '{v['description']}', '{v['created_at']}')\"))\n",
    "                for k, v in test_subjects.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_CLASS,\n",
    "            [\n",
    "                (INSERT_CLASS + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO classes (id, name, teacher_id, created_at) VALUES ({k}, '{v['name']}', {v['teacher_id']}, '{v['created_at']}')\"))\n",
    "                for k, v in test_classes.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_STUDENT,\n",
    "            [\n",
    "                (INSERT_STUDENT + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO students (id, first_name, last_name, birth_date, created_at) VALUES ({k}, '{v['first_name']}', '{v['last_name']}', '{v['birth_date']}', '{v['created_at']}')\"))\n",
    "                for k, v in test_students.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_ENROLLMENT,\n",
    "            [\n",
    "                (INSERT_ENROLLMENT + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES ({k[0]}, {k[1]}, '{v['enrolled_at']}')\"))\n",
    "                for k, v in test_enrollments.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_GRADE,\n",
    "            [\n",
    "                (INSERT_GRADE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES ({k}, {v['student_id']}, {v['subject_id']}, {v['grade']}, '{v['created_at']}')\"))\n",
    "                for k, v in test_grades.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SCHEDULE,\n",
    "            [\n",
    "                (INSERT_SCHEDULE + str(k), lambda k=k, v=v: postgres_operation(postgres_client,\n",
    "                    f\"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES ({k}, {v['class_id']}, {v['subject_id']}, '{v['day_of_week']}', '{v['time_start']}', '{v['time_end']}')\"))\n",
    "                for k, v in test_schedules.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_STUDENT, [\n",
    "                (SELECT_STUDENT + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM students WHERE id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_CLASS, [\n",
    "                (SELECT_CLASS + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM classes WHERE id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER, [\n",
    "                (SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM students s JOIN enrollments e ON s.id = e.student_id JOIN classes c ON e.class_id = c.id WHERE c.teacher_id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT, [\n",
    "                (SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM schedules s JOIN classes c ON s.class_id = c.id JOIN enrollments e ON c.id = e.class_id WHERE e.student_id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS, [\n",
    "                (SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"SELECT * FROM grades g JOIN subjects su ON g.subject_id = su.id JOIN students s ON g.student_id = s.id JOIN enrollments e ON s.id = e.student_id JOIN classes c ON e.class_id = c.id WHERE c.id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_STUDENT_NAME, [\n",
    "                (UPDATE_STUDENT_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE students SET last_name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_GRADES_FOR_STUDENT, [\n",
    "                (UPDATE_ALL_GRADES_FOR_STUDENT + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE grades SET grade = 100 WHERE student_id = {k} AND subject_id = 10000011\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_CLASS_NAME, [\n",
    "                (UPDATE_CLASS_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE classes SET name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TEACHER_LAST_NAME, [\n",
    "                (UPDATE_TEACHER_LAST_NAME + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE teachers SET last_name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM, [\n",
    "                (UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"UPDATE subjects SET description = 'UPDATED' WHERE id IN (SELECT subject_id FROM grades WHERE student_id = {k})\"))\n",
    "                for k in random_numbers_list\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_STUDENT, [\n",
    "                (DELETE_STUDENT + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM students WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_CLASS, [\n",
    "                (DELETE_CLASS + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM classes WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER, [\n",
    "                (DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM subjects WHERE id IN (SELECT subject_id FROM schedules WHERE class_id IN (SELECT id FROM classes WHERE teacher_id = {k}))\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_TEACHER_WHO_TAUGHT_STUDENT, [\n",
    "                (DELETE_TEACHER_WHO_TAUGHT_STUDENT + str(k), lambda k=k: postgres_operation(postgres_client,\n",
    "                    f\"DELETE FROM teachers WHERE id IN (SELECT teacher_id FROM classes WHERE id IN (SELECT class_id FROM enrollments WHERE student_id = {k}))\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_scenarios(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_scenarios(select_scenarios)\n",
    "    benchmark.run_scenarios(update_scenarios)\n",
    "    benchmark.run_scenarios(delete_scenarios)\n",
    "    return benchmark.get_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mariadb_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = SimpleBenchmark(\"mariadb\", data_dir)\n",
    "    def setup_for_insert():\n",
    "        with open('schemas/mariadb_schema.sql', 'r') as f:\n",
    "            sql_schema = f.read()\n",
    "\n",
    "        initialize_mariadb_schema(mariadb_client, sql_schema)\n",
    "        load_mariadb_data(mariadb_client, data_dir)\n",
    "\n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TEACHER,\n",
    "            [\n",
    "                (INSERT_TEACHER + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client, \n",
    "                    f\"INSERT INTO teachers (id, first_name, last_name, subject, hire_date, created_at) VALUES ({k}, '{v['first_name']}', '{v['last_name']}', '{v['subject']}', '{v['hire_date']}', '{v['created_at']}')\")) \n",
    "                for k, v in test_teachers.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SUBJECT,\n",
    "            [\n",
    "                (INSERT_SUBJECT + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO subjects (id, name, description, created_at) VALUES ({k}, '{v['name']}', '{v['description']}', '{v['created_at']}')\"))\n",
    "                for k, v in test_subjects.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_CLASS,\n",
    "            [\n",
    "                (INSERT_CLASS + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO classes (id, name, teacher_id, created_at) VALUES ({k}, '{v['name']}', {v['teacher_id']}, '{v['created_at']}')\"))\n",
    "                for k, v in test_classes.items()\n",
    "            ]\n",
    "        ), \n",
    "        (\n",
    "            INSERT_STUDENT,\n",
    "            [\n",
    "                (INSERT_STUDENT + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO students (id, first_name, last_name, birth_date, created_at) VALUES ({k}, '{v['first_name']}', '{v['last_name']}', '{v['birth_date']}', '{v['created_at']}')\"))\n",
    "                for k, v in test_students.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_ENROLLMENT,\n",
    "            [\n",
    "                (INSERT_ENROLLMENT + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES ({k[0]}, {k[1]}, '{v['enrolled_at']}')\"))\n",
    "                for k, v in test_enrollments.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_GRADE,\n",
    "            [\n",
    "                (INSERT_GRADE + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES ({k}, {v['student_id']}, {v['subject_id']}, {v['grade']}, '{v['created_at']}')\"))\n",
    "                for k, v in test_grades.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SCHEDULE,\n",
    "            [\n",
    "                (INSERT_SCHEDULE + str(k), lambda k=k, v=v: mariadb_operation(mariadb_client,\n",
    "                    f\"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES ({k}, {v['class_id']}, {v['subject_id']}, '{v['day_of_week']}', '{v['time_start']}', '{v['time_end']}')\"))\n",
    "                for k, v in test_schedules.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_STUDENT, [\n",
    "                (SELECT_STUDENT + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"SELECT * FROM students WHERE id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_CLASS, [\n",
    "                (SELECT_CLASS + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"SELECT * FROM classes WHERE id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER, [\n",
    "                (SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"SELECT * FROM students s JOIN enrollments e ON s.id = e.student_id JOIN classes c ON e.class_id = c.id WHERE c.teacher_id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT, [\n",
    "                (SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"SELECT * FROM schedules s JOIN classes c ON s.class_id = c.id JOIN enrollments e ON c.id = e.class_id WHERE e.student_id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS, [\n",
    "                (SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"SELECT * FROM grades g JOIN subjects su ON g.subject_id = su.id JOIN students s ON g.student_id = s.id JOIN enrollments e ON s.id = e.student_id JOIN classes c ON e.class_id = c.id WHERE c.id = {k}\", fetch=True))\n",
    "                for k in random_numbers_list\n",
    "            ] \n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_STUDENT_NAME, [\n",
    "                (UPDATE_STUDENT_NAME + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"UPDATE students SET last_name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_GRADES_FOR_STUDENT, [\n",
    "                (UPDATE_ALL_GRADES_FOR_STUDENT + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"UPDATE grades SET grade = 100 WHERE student_id = {k} AND subject_id = 10000011\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_CLASS_NAME, [\n",
    "                (UPDATE_CLASS_NAME + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"UPDATE classes SET name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TEACHER_LAST_NAME, [\n",
    "                (UPDATE_TEACHER_LAST_NAME + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"UPDATE teachers SET last_name = 'UPDATED' WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM, [\n",
    "                (UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"UPDATE subjects SET description = 'UPDATED' WHERE id IN (SELECT subject_id FROM grades WHERE student_id = {k})\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_STUDENT, [\n",
    "                (DELETE_STUDENT + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"DELETE FROM students WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_CLASS, [\n",
    "                (DELETE_CLASS + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"DELETE FROM classes WHERE id = {k}\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER, [\n",
    "                (DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"DELETE FROM subjects WHERE id IN (SELECT subject_id FROM schedules WHERE class_id IN (SELECT id FROM classes WHERE teacher_id = {k}))\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_TEACHER_WHO_TAUGHT_STUDENT, [\n",
    "                (DELETE_TEACHER_WHO_TAUGHT_STUDENT + str(k), lambda k=k: mariadb_operation(mariadb_client,\n",
    "                    f\"DELETE FROM teachers WHERE id IN (SELECT teacher_id FROM classes WHERE id IN (SELECT class_id FROM enrollments WHERE student_id = {k}))\"))\n",
    "                for k in random_numbers_list\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    benchmark.run_scenarios(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_scenarios(select_scenarios)\n",
    "    benchmark.run_scenarios(update_scenarios)\n",
    "    benchmark.run_scenarios(delete_scenarios)\n",
    "    return benchmark.get_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e57b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = SimpleBenchmark(\"mongodb\", data_dir)\n",
    "    \n",
    "    def setup_for_insert():\n",
    "        # Initialize MongoDB schema - create collections and indexes\n",
    "        initialize_mongo_schema(mongo_client)\n",
    "        # Load data from CSV files\n",
    "        load_mongo_data(mongo_client, data_dir)\n",
    "    \n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TEACHER,\n",
    "            [\n",
    "                (INSERT_TEACHER + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'teachers', 'insert',\n",
    "                    data={\n",
    "                        \"_id\": k,\n",
    "                        \"first_name\": v['first_name'],\n",
    "                        \"last_name\": v['last_name'],\n",
    "                        \"subject\": v['subject'],\n",
    "                        \"hire_date\": v['hire_date'],\n",
    "                        \"created_at\": v['created_at']\n",
    "                    }\n",
    "                )) for k, v in test_teachers.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SUBJECT,\n",
    "            [\n",
    "                (INSERT_SUBJECT + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'subjects', 'insert',\n",
    "                    data={\n",
    "                        \"_id\": k,\n",
    "                        \"name\": v['name'],\n",
    "                        \"description\": v['description'],\n",
    "                        \"created_at\": v['created_at']\n",
    "                    }\n",
    "                )) for k, v in test_subjects.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_CLASS,\n",
    "            [\n",
    "                (INSERT_CLASS + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'insert',\n",
    "                    data={\n",
    "                        \"_id\": k,\n",
    "                        \"name\": v['name'],\n",
    "                        \"teacher_id\": v['teacher_id'],\n",
    "                        \"created_at\": v['created_at']\n",
    "                    }\n",
    "                )) for k, v in test_classes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_STUDENT,\n",
    "            [\n",
    "                (INSERT_STUDENT + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'insert',\n",
    "                    data={\n",
    "                        \"_id\": k,\n",
    "                        \"first_name\": v['first_name'],\n",
    "                        \"last_name\": v['last_name'],\n",
    "                        \"birth_date\": v['birth_date'],\n",
    "                        \"created_at\": v['created_at'],\n",
    "                        \"enrollments\": [],\n",
    "                        \"grades\": []\n",
    "                    }\n",
    "                )) for k, v in test_students.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_ENROLLMENT,\n",
    "            [\n",
    "                (INSERT_ENROLLMENT + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'update',\n",
    "                    query={\"_id\": k[0]},\n",
    "                    data={\"$push\": {\"enrollments\": {\n",
    "                        \"class_id\": k[1],\n",
    "                        \"enrolled_at\": v['enrolled_at']\n",
    "                    }}}\n",
    "                )) for k, v in test_enrollments.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_GRADE,\n",
    "            [\n",
    "                (INSERT_GRADE + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'update',\n",
    "                    query={\"_id\": v['student_id']},\n",
    "                    data={\"$push\": {\"grades\": {\n",
    "                        \"grade_id\": k,\n",
    "                        \"subject_id\": v['subject_id'],\n",
    "                        \"grade\": v['grade'],\n",
    "                        \"created_at\": v['created_at']\n",
    "                    }}}\n",
    "                )) for k, v in test_grades.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SCHEDULE,\n",
    "            [\n",
    "                (INSERT_SCHEDULE + str(k), lambda k=k, v=v: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'update',\n",
    "                    query={\"_id\": v['class_id']},\n",
    "                    data={\"$push\": {\"schedule\": {\n",
    "                        \"schedule_id\": k,\n",
    "                        \"subject_id\": v['subject_id'],\n",
    "                        \"day_of_week\": v['day_of_week'],\n",
    "                        \"time_start\": v['time_start'],\n",
    "                        \"time_end\": v['time_end']\n",
    "                    }}}\n",
    "                )) for k, v in test_schedules.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_STUDENT, [\n",
    "                (SELECT_STUDENT + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'find',\n",
    "                    query={\"_id\": k},\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_CLASS, [\n",
    "                (SELECT_CLASS + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'find',\n",
    "                    query={\"_id\": k},\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER, [\n",
    "                (SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'find',\n",
    "                    query={\"enrollments.class_id\": {\"$in\": \n",
    "                            list(mongo_client['benchmark']['classes'].find({\"teacher_id\": k}, {\"_id\": 1}))\n",
    "                    }},\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT, [\n",
    "                (SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'find',\n",
    "                    query={\n",
    "                        \"_id\": {\"$in\": list(\n",
    "                            map(lambda x: x[\"class_id\"], \n",
    "                                mongo_client['benchmark']['students'].find_one({\"_id\": k}, {\"enrollments\": 1})[\"enrollments\"]\n",
    "                            )\n",
    "                        )}\n",
    "                    },\n",
    "                    options={\"projection\": {\"schedule\": 1, \"name\": 1}},\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS, [\n",
    "                (SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'find',\n",
    "                    query={\"enrollments.class_id\": k},\n",
    "                    options={\n",
    "                        \"projection\": {\n",
    "                            \"first_name\": 1, \n",
    "                            \"last_name\": 1, \n",
    "                            \"grades\": 1\n",
    "                        }\n",
    "                    },\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_STUDENT_NAME, [\n",
    "                (UPDATE_STUDENT_NAME + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'update',\n",
    "                    query={\"_id\": k},\n",
    "                    data={\"$set\": {\"last_name\": \"UPDATED\"}}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_GRADES_FOR_STUDENT, [\n",
    "                (UPDATE_ALL_GRADES_FOR_STUDENT + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'update',\n",
    "                    query={\n",
    "                        \"_id\": k,\n",
    "                        \"grades.subject_id\": 10000011\n",
    "                    },\n",
    "                    data={\"$set\": {\"grades.$.grade\": 100}}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_CLASS_NAME, [\n",
    "                (UPDATE_CLASS_NAME + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'update',\n",
    "                    query={\"_id\": k},\n",
    "                    data={\"$set\": {\"name\": \"UPDATED\"}}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TEACHER_LAST_NAME, [\n",
    "                (UPDATE_TEACHER_LAST_NAME + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'teachers', 'update',\n",
    "                    query={\"_id\": k},\n",
    "                    data={\"$set\": {\"last_name\": \"UPDATED\"}}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM, [\n",
    "                (UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM + str(k), lambda k=k: \n",
    "                    # First get all subject IDs that the student has grades for\n",
    "                    [mongo_operation(\n",
    "                        mongo_client, 'benchmark', 'subjects', 'update',\n",
    "                        query={\"_id\": subject_id},\n",
    "                        data={\"$set\": {\"description\": \"UPDATED\"}}\n",
    "                    ) for subject_id in list(map(\n",
    "                        lambda x: x[\"subject_id\"], \n",
    "                        mongo_client['benchmark']['students'].find_one({\"_id\": k}, {\"grades.subject_id\": 1})[\"grades\"]\n",
    "                    ))]\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_STUDENT, [\n",
    "                (DELETE_STUDENT + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'students', 'delete',\n",
    "                    query={\"_id\": k}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_CLASS, [\n",
    "                (DELETE_CLASS + str(k), lambda k=k: mongo_operation(\n",
    "                    mongo_client, 'benchmark', 'classes', 'delete',\n",
    "                    query={\"_id\": k}\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER, [\n",
    "                (DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER + str(k), lambda k=k: \n",
    "                    # Find classes taught by teacher, then get all subject IDs from those classes' schedules\n",
    "                    [mongo_operation(\n",
    "                        mongo_client, 'benchmark', 'subjects', 'delete',\n",
    "                        query={\"_id\": {\"$in\": list(set([\n",
    "                            schedule[\"subject_id\"] for class_doc in \n",
    "                            mongo_client['benchmark']['classes'].find({\"teacher_id\": k}) \n",
    "                            for schedule in class_doc.get(\"schedule\", [])\n",
    "                        ]))}}\n",
    "                    )]\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_TEACHER_WHO_TAUGHT_STUDENT, [\n",
    "                (DELETE_TEACHER_WHO_TAUGHT_STUDENT + str(k), lambda k=k:\n",
    "                    # Find student's enrolled class IDs, then find teachers of those classes\n",
    "                    (lambda student_doc: \n",
    "                        [mongo_operation(\n",
    "                            mongo_client, 'benchmark', 'teachers', 'delete',\n",
    "                            query={\"_id\": {\"$in\": list(set([\n",
    "                                class_doc[\"teacher_id\"] for class_id in \n",
    "                                [enroll[\"class_id\"] for enroll in student_doc.get(\"enrollments\", [])]\n",
    "                                for class_doc in mongo_client['benchmark']['classes'].find({\"_id\": class_id})\n",
    "                            ]))}} if student_doc else {}\n",
    "                        )]\n",
    "                    )(mongo_client['benchmark']['students'].find_one({\"_id\": k}, {\"enrollments\": 1}))\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    benchmark.run_scenarios(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_scenarios(select_scenarios)\n",
    "    benchmark.run_scenarios(update_scenarios)\n",
    "    benchmark.run_scenarios(delete_scenarios)\n",
    "    \n",
    "    return benchmark.get_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cassandra_benchmark(data_dir, random_numbers_list) -> pd.DataFrame:\n",
    "    benchmark = SimpleBenchmark(\"cassandra\", data_dir)\n",
    "    \n",
    "    def setup_for_insert():\n",
    "        # Initialize Cassandra schema\n",
    "        initialize_cassandra_schema(cassandra_session)\n",
    "        # Load data from CSV files\n",
    "        load_cassandra_data(cassandra_session, data_dir)\n",
    "    \n",
    "    # INSERT scenarios\n",
    "    insert_scenarios = [\n",
    "        (\n",
    "            INSERT_TEACHER,\n",
    "            [\n",
    "                (INSERT_TEACHER + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session, \n",
    "                    \"INSERT INTO teachers (id, first_name, last_name) VALUES (?, ?, ?)\",\n",
    "                    params=[k, v['first_name'], v['last_name']]\n",
    "                )) for k, v in test_teachers.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SUBJECT,\n",
    "            [\n",
    "                (INSERT_SUBJECT + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO subjects (id, name, description) VALUES (?, ?, ?)\",\n",
    "                    params=[k, v['name'], v['description']]\n",
    "                )) for k, v in test_subjects.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_CLASS,\n",
    "            [\n",
    "                (INSERT_CLASS + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO classes (id, name, teacher_id) VALUES (?, ?, ?)\",\n",
    "                    params=[k, v['name'], v['teacher_id']]\n",
    "                )) for k, v in test_classes.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_STUDENT,\n",
    "            [\n",
    "                (INSERT_STUDENT + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO students (id, first_name, last_name, birth_date) VALUES (?, ?, ?, ?)\",\n",
    "                    params=[k, v['first_name'], v['last_name'], v['birth_date']]\n",
    "                )) for k, v in test_students.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_ENROLLMENT,\n",
    "            [\n",
    "                (INSERT_ENROLLMENT + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO enrollments (student_id, class_id, enrolled_at) VALUES (?, ?, ?)\",\n",
    "                    params=[k[0], k[1], v['enrolled_at']]\n",
    "                )) for k, v in test_enrollments.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_GRADE,\n",
    "            [\n",
    "                (INSERT_GRADE + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO grades (id, student_id, subject_id, grade, created_at) VALUES (?, ?, ?, ?, ?)\",\n",
    "                    params=[k, v['student_id'], v['subject_id'], v['grade'], v['created_at']]\n",
    "                )) for k, v in test_grades.items()\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            INSERT_SCHEDULE,\n",
    "            [\n",
    "                (INSERT_SCHEDULE + str(k), lambda k=k, v=v: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"INSERT INTO schedules (id, class_id, subject_id, day_of_week, time_start, time_end) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                    # Convert day name to integer using day_map\n",
    "                    params=[k, v['class_id'], v['subject_id'], \n",
    "                            {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}.get(v['day_of_week'], 0), \n",
    "                            v['time_start'], v['time_end']]\n",
    "                )) for k, v in test_schedules.items()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # SELECT scenarios\n",
    "    select_scenarios = [\n",
    "        (\n",
    "            SELECT_STUDENT, [\n",
    "                (SELECT_STUDENT + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"SELECT * FROM students WHERE id = ?\",\n",
    "                    params=[k],\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_CLASS, [\n",
    "                (SELECT_CLASS + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"SELECT * FROM classes WHERE id = ?\",\n",
    "                    params=[k],\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER, [\n",
    "                (SELECT_ALL_STUDENTS_TAUGHT_BY_TEACHER + str(k), lambda k=k: cassandra_operation(\n",
    "                    # Using denormalized student_enrollments table to get all students for a teacher\n",
    "                    cassandra_session,\n",
    "                    \"SELECT s.* FROM students s JOIN student_enrollments se ON s.id = se.student_id WHERE se.teacher_id = ? ALLOW FILTERING\",\n",
    "                    params=[k],\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT, [\n",
    "                (SELECT_ALL_SCHEDULES_FOR_A_SPECIFIC_STUDENT + str(k), lambda k=k: cassandra_operation(\n",
    "                    # Using denormalized student_enrollments and schedules tables\n",
    "                    cassandra_session,\n",
    "                    \"\"\"\n",
    "                    SELECT sc.* FROM schedules sc \n",
    "                    JOIN student_enrollments se ON sc.class_id = se.class_id \n",
    "                    WHERE se.student_id = ? ALLOW FILTERING\n",
    "                    \"\"\",\n",
    "                    params=[k],\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS, [\n",
    "                (SELECT_ALL_GRADES_WITH_SUBJECT_DESCRIPTIONS_FOR_ALL_STUDENTS_IN_THE_CLASS + str(k), lambda k=k: cassandra_operation(\n",
    "                    # Using denormalized student_grades table\n",
    "                    cassandra_session,\n",
    "                    \"\"\"\n",
    "                    SELECT sg.* FROM student_grades sg \n",
    "                    JOIN student_enrollments se ON sg.student_id = se.student_id \n",
    "                    WHERE se.class_id = ? ALLOW FILTERING\n",
    "                    \"\"\",\n",
    "                    params=[k],\n",
    "                    fetch=True\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # UPDATE scenarios\n",
    "    update_scenarios = [\n",
    "        (\n",
    "            UPDATE_STUDENT_NAME, [\n",
    "                (UPDATE_STUDENT_NAME + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"UPDATE students SET last_name = 'UPDATED' WHERE id = ?\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_ALL_GRADES_FOR_STUDENT, [\n",
    "                (UPDATE_ALL_GRADES_FOR_STUDENT + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"UPDATE grades SET grade = 100 WHERE student_id = ? AND subject_id = 10000011\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_CLASS_NAME, [\n",
    "                (UPDATE_CLASS_NAME + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"UPDATE classes SET name = 'UPDATED' WHERE id = ?\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_TEACHER_LAST_NAME, [\n",
    "                (UPDATE_TEACHER_LAST_NAME + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"UPDATE teachers SET last_name = 'UPDATED' WHERE id = ?\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM, [\n",
    "                (UPDATE_SUBJECT_DESCRIPTION_FOR_ALL_SUBJECTS_THAT_STUDENTS_HAS_GRADES_FROM + str(k), lambda k=k: \n",
    "                    # First get all subject IDs that the student has grades for\n",
    "                    cassandra_operation(\n",
    "                        cassandra_session,\n",
    "                        \"UPDATE subjects SET description = 'UPDATED' WHERE id IN (SELECT subject_id FROM student_grades WHERE student_id = ? ALLOW FILTERING)\",\n",
    "                        params=[k]\n",
    "                    )\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # DELETE scenarios\n",
    "    delete_scenarios = [\n",
    "        (\n",
    "            DELETE_STUDENT, [\n",
    "                (DELETE_STUDENT + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"DELETE FROM students WHERE id = ?\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_CLASS, [\n",
    "                (DELETE_CLASS + str(k), lambda k=k: cassandra_operation(\n",
    "                    cassandra_session,\n",
    "                    \"DELETE FROM classes WHERE id = ?\",\n",
    "                    params=[k]\n",
    "                )) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER, [\n",
    "                (DELETE_SUBJECTS_THAT_ARE_IN_THE_SCHEDULE_OF_THE_TEACHER + str(k), lambda k=k: \n",
    "                    # Need to get subject IDs from schedules where class has teacher_id = k\n",
    "                    cassandra_operation(\n",
    "                        cassandra_session,\n",
    "                        \"\"\"\n",
    "                        DELETE FROM subjects WHERE id IN (\n",
    "                            SELECT subject_id FROM schedules \n",
    "                            WHERE class_id IN (\n",
    "                                SELECT id FROM classes WHERE teacher_id = ? ALLOW FILTERING\n",
    "                            ) ALLOW FILTERING\n",
    "                        )\n",
    "                        \"\"\",\n",
    "                        params=[k]\n",
    "                    )\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "        (\n",
    "            DELETE_TEACHER_WHO_TAUGHT_STUDENT, [\n",
    "                (DELETE_TEACHER_WHO_TAUGHT_STUDENT + str(k), lambda k=k:\n",
    "                    # Find teacher IDs from student_enrollments\n",
    "                    cassandra_operation(\n",
    "                        cassandra_session,\n",
    "                        \"DELETE FROM teachers WHERE id IN (SELECT teacher_id FROM student_enrollments WHERE student_id = ? ALLOW FILTERING)\",\n",
    "                        params=[k]\n",
    "                    )\n",
    "                ) for k in random_numbers_list\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Run all benchmark scenarios\n",
    "    benchmark.run_scenarios(insert_scenarios, setup_method=setup_for_insert)\n",
    "    benchmark.run_scenarios(select_scenarios)\n",
    "    benchmark.run_scenarios(update_scenarios)\n",
    "    benchmark.run_scenarios(delete_scenarios)\n",
    "    \n",
    "    return benchmark.get_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ba94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "rand_10_between_1_100 = sample(range(1, 101), 10)\n",
    "postgres_results_df = postgres_benchmark(scale_100_dir, rand_10_between_1_100)\n",
    "mariadb_results_df = mariadb_benchmark(scale_100_dir, rand_10_between_1_100)\n",
    "mongo_results_df = mongo_benchmark(scale_100_dir, rand_10_between_1_100)\n",
    "cassandra_results_df = cassandra_benchmark(scale_100_dir, rand_10_between_1_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb99406",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([postgres_results_df, mariadb_results_df, mongo_results_df, cassandra_results_df], ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_database_performance(\n",
    "    df, \n",
    "    keyword=None,\n",
    "    metric='total_time', \n",
    "    data_dir=None, \n",
    "    figsize=(14, 8), \n",
    "    title=None,\n",
    "    sort_by=None,\n",
    "    log_scale=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a performance comparison plot for different databases.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The merged DataFrame containing all benchmark results\n",
    "    keyword : str, optional\n",
    "        Filter scenarios containing this keyword\n",
    "    metric : str, default='total_time'\n",
    "        The metric to plot. Must be one of:\n",
    "        'total_time', 'operations', 'avg_operation_time', 'throughput',\n",
    "        'cpu_avg', 'memory_avg', 'disk_read_mb', 'disk_write_mb'\n",
    "    data_dir : str, optional\n",
    "        Filter by specific data directory\n",
    "    figsize : tuple, default=(14, 8)\n",
    "        Figure size as (width, height)\n",
    "    title : str, optional\n",
    "        Custom title for the plot\n",
    "    sort_by : str, optional\n",
    "        Sort scenarios by: 'name', 'value', or None for default ordering\n",
    "    log_scale : bool, default=False\n",
    "        Use logarithmic scale for the metric axis\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax: matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "    # Validate metric\n",
    "    valid_metrics = ['total_time', 'operations', 'avg_operation_time', 'throughput',\n",
    "                    'cpu_avg', 'memory_avg', 'disk_read_mb', 'disk_write_mb']\n",
    "    \n",
    "    if metric not in valid_metrics:\n",
    "        raise ValueError(f\"Invalid metric: {metric}. Must be one of {valid_metrics}\")\n",
    "    \n",
    "    # Clone dataframe to avoid modifying the original\n",
    "    plot_df = df.copy()\n",
    "    \n",
    "    # Apply filters\n",
    "    if data_dir:\n",
    "        plot_df = plot_df[plot_df['data_dir'] == data_dir]\n",
    "    \n",
    "    if keyword:\n",
    "        plot_df = plot_df[plot_df['scenario'].str.contains(keyword, case=False)]\n",
    "    \n",
    "    if plot_df.empty:\n",
    "        print(\"No data matches the specified filters.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get unique databases and scenarios\n",
    "    databases = plot_df['database'].unique()\n",
    "    scenarios = plot_df['scenario'].unique()\n",
    "    \n",
    "    # Sort scenarios if requested\n",
    "    if sort_by == 'name':\n",
    "        scenarios = sorted(scenarios)\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Define color palette\n",
    "    palette = sns.color_palette(\"husl\", len(scenarios))\n",
    "    \n",
    "    # Calculate bar width based on number of scenarios\n",
    "    bar_width = 0.8 / len(scenarios)\n",
    "    \n",
    "    # Create bars for each scenario\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        # Get data for this scenario\n",
    "        scenario_data = plot_df[plot_df['scenario'] == scenario]\n",
    "        \n",
    "        # Calculate x positions for this scenario\n",
    "        x = np.arange(len(databases))\n",
    "        offset = (i - len(scenarios)/2 + 0.5) * bar_width\n",
    "        \n",
    "        # Get values for each database for this metric and scenario\n",
    "        values = []\n",
    "        for db in databases:\n",
    "            val = scenario_data[scenario_data['database'] == db][metric].values\n",
    "            values.append(val[0] if len(val) > 0 else np.nan)\n",
    "        \n",
    "        # Plot bars\n",
    "        ax.bar(x + offset, values, width=bar_width, label=scenario, color=palette[i])\n",
    "    \n",
    "    # Set logarithmic scale if requested\n",
    "    if log_scale and all(v > 0 for v in plot_df[metric].values):\n",
    "        ax.set_yscale('log')\n",
    "    \n",
    "    # Customize plot\n",
    "    if not title:\n",
    "        title = f\"{metric.replace('_', ' ').title()} by Database and Scenario\"\n",
    "        if keyword:\n",
    "            title += f\" (filtered: '{keyword}')\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Database', fontsize=14)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=14)\n",
    "    ax.set_xticks(np.arange(len(databases)))\n",
    "    ax.set_xticklabels(databases, rotation=45, ha='right')\n",
    "    \n",
    "    # Add legend with smaller font and outside the plot area\n",
    "    ax.legend(title=\"Scenarios\", bbox_to_anchor=(0.5, -0.15), loc='upper center', \n",
    "          fontsize=10, ncol=min(len(scenarios), 3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_database_performance(\n",
    "    merged_df,\n",
    "    keyword=\"INSERT\",\n",
    "    metric=\"total_time\",\n",
    "    log_scale=False\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_database_performance(\n",
    "    merged_df,\n",
    "    keyword=\"SELECT\",\n",
    "    metric=\"total_time\",\n",
    "    log_scale=False\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_database_performance(\n",
    "    merged_df,\n",
    "    keyword=\"UPDATE\",\n",
    "    metric=\"total_time\",\n",
    "    log_scale=False\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_database_performance(\n",
    "    merged_df,\n",
    "    keyword=\"DELETE\",\n",
    "    metric=\"total_time\",\n",
    "    log_scale=False\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
